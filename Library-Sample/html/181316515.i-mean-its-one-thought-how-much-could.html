<p>From 1935 to 1936, a small New York press sold a book with the incurably pulpy title <em>The Banana Empire: A Case Study of Economic Imperialism</em>. It is the sort of book you imagine moldering on the bottom shelf of an economics department library, spine sun-bleached to illegibility, quietly judging generations of graduate students who meant to read it and never did. Written by Charles Kepner, a Columbia academic, and Jay Soothill, a former administrator for United Fruit in Costa Rica and Panama, it was one of the first attempts to treat a single vertically integrated company as a planetary weather system: railroads, shipping lines, plantations, commissary stores, tax codes, and coup attempts all bent around the gravitational pull of a snack.</p><p>The book is mostly out of print and almost entirely unread, but it sits there at the beginning of something. When people talk about globalization now, they like to point at fiber optics and derivatives and “aspatial” quaternary-sector capital, to the way firms can push buttons in one timezone and fire people in another. Kepner and Soothill, writing before radar was standard on ships, were already describing a corporation that behaved like a distributed machine: a Boston head office pushing instructions down telegraph lines into tropical lowlands, rail subsidiaries laying track purely to control who could move freight, and the “Great White Fleet” of refrigerated steamers gliding out of Central American ports on schedule whether or not their holds were full. Bananas were only nominally the subject. The real topic was how you turn a plant that rots in a week into something that behaves like a standardized, reliable unit of trade.</p><p>Bananas are good for this kind of experiment. They are, like most flora and fauna on this god-forsaken planet, weird upon observation—giant herbs that only fruit once, their stems a stack of rolled leaves—but economically ideal. Each plant gives you a single bunch, nine “hands,” fifty to sixty-odd pounds of fruit; the plantations run year-round, rain or shine, so there is always cargo for the tramlines and the ships. To make this work, United Fruit and its peers built what were essentially fruit logistics computers across the Caribbean coast: irrigation grids and narrow-gauge rail, telephones to synchronize cutting orders with ship arrivals, hospitals and barracks for imported workers, and a network of company-owned commissaries that converted wages back into profit before they ever left the zone. In the language of the time, this was just “development.” From a distance, it looks more like someone teaching a landscape to think in terms of bunches per acre and cents per mile.</p><p>Meanwhile, Lucille Bluth stares at her son with the bland incomprehension of old money and asks: “I mean, it’s one banana, Michael, how much could it cost? Ten dollars?” The joke lands because it’s absurd to be so disconnected from the reality. But the joke only works because, for a century, companies like United Fruit spent an obscene amount of effort and occasional gunfire making sure that a banana could be treated as a number: a negligible, dependable unit, sliding down chutes from plantation to port to grocery shelf without anyone having to think about it very hard.</p><p>--</p><p>What Kepner and Soothill stumbled into—through shipping logs and plantation ledgers rather than dialectical fury—was commodities. Not the sad little econ-101 definition (“things you can buy and sell”) that turns up in textbooks, but the older and more interesting notion of a commodity as an achievement, almost a kind of spell. You take something stubborn and particular and perishable—this banana, from that field, cut on a Thursday after three days of rain—and you subject it to a series of operations that strip away everything that makes it itself until what remains is a unit the market can stand to look at. A commodity is not a type of object; it is a procedure for making objects behave as if they were all the same.</p><p>The banana is a good place to watch this happen in real time. Left to its own devices, it is an aggressively mortal thing. You buy one in the morning; by the end of the week it has collapsed into a bruise, leaking sugar onto the counter. The commodity banana is something else entirely. It has been graded at the siding, sorted by curvature and size and the presence or absence of cosmetic blemishes; wrapped in plastic sleeves to slow down ethylene; reported by telegraph as “Forty tons, green, Class B”; packed into refrigerators that glide through the Caribbean oblivious to storms. The foreman might still wake up at night worrying about disease and drought, but by the time the fruit arrives at the exchange in New York or Boston, what matters is not the plants or the weather or the workers. What matters is the grade, the weight, and the price on a futures contract somebody wrote a month earlier. The bananas have been translated from living things into integers.</p><p>Grain went first, because grain is what you experiment on when you are a nineteenth-century American determined to turn the prairie into a spreadsheet. Before the grain elevator, “wheat” was a local concept: this farmer’s harvest from that field in that year. After the elevator, wheat became No. 1 or No. 2 Spring Wheat, anonymous loads poured into towering silos and stirred together until all that was left of each crop was its contribution to a uniform mass. The genius of the elevator was that it disguised an act of epistemic violence as progress: you dump a thousand distinct harvests into the machinery and receive an unmarked slurry traders can swap without ever sullying their hands with an actual kernel. Timber went through the same rite; crude oil had to learn to answer to the word “barrel” long after the last actual barrel was retired in favor of tanks and pipelines. Even labor—the most wildly idiosyncratic thing in sight—was attacked by the apparatus and carved into “labor hours” and “full-time equivalents,” then shuffled into Polanyi’s uneasy category of fictitious commodities, the things that were never meant to be priced and somehow are.</p><p>If you really want to annoy yourself, you can spend an afternoon with anthropologists cataloguing the “social life of things,” as Appadurai and Kopytoff do, tracing all the ways objects fall into and out of commodity status, acquire biographies, get recycled as heirlooms or ritual gifts. It’s diverting, but for our purposes the choreography matters more than the costumes. The crucial steps are always the same. First you simplify: you shave off attributes and insist that only this one variable—protein content, board-footage, octane, hours worked—matters. Then you specify a unit: bushel, ton, barrel, kilowatt-hour. Then you build the plumbing: elevators, telegraph lines, exchanges, paperwork, inspectors, standardized contracts. Only when the thing can flow through that plumbing without demanding further explanation does it truly become a commodity.</p><p>The commodity, in other words, is not the banana or the bushel or the barrel. The commodity is the infrastructure that makes bananas and bushels and barrels tradable.</p><p>Bananas seem comic only because the infrastructure was so theatrical—rail lines curled along the coast like nervous systems, telegraph operators calculating the exact ripening window of entire provinces, presidents wobbling on the fulcrum of shipping schedules. But structurally, the miracle is the same as it ever was. You take something messy and local and loud with its own life, and you scrub it until it flows through pipes. We have learned how to do this to plants, to minerals, to time, to the air itself. It would, in retrospect, be stranger if we did not eventually attempt to do it to thought.</p><p>Now, instead of explaining why it matters that we are making so-called “intelligence” (have you read this essay?) to behave like No. 2 Spring Wheat, let’s make sure to confirm that the “technologists” on Substack arguing about AI are wrong, as in, not even asking the right question. Instead of asking how you standardize and ship thinking, they are asking whether the bananas are secretly alive. Do you see how ridiculous this sounds?</p><p>Without any proper consideration, our feeds would suggest we are in some sort of life-or-death struggle between two ways of understanding these new systems. On one side, the skeptics, who insist that large language models are nothing but stochastic parrots: glorified autocomplete engines that string together fragments of text in ways that mimic intelligence without ever touching it. Trained on stolen books and scraped forums, they merely cough up the statistical average of whatever poor material has been shoveled into them. These people like to remind you that prediction is not understanding, that a good forger does not become the original artist, that parrots repeat what they cannot mean.</p><p>On the other side, the enthusiasts, who see in the same architectures the larval form of a new kind of mind. To them, these models are proto-persons: vast alien subjectivities made of floating-point operations, only a few training runs away from self-awareness. If they are dangerous, it is not because they are stupid but because they are, or soon will be, much too smart—entities whose preferences we will need to “align” as carefully as any treaty with a nuclear-armed neighbor. Sometimes these people simply want a friend who can talk forever; sometimes they want a god.</p><p>Peer closely at this picture and a few troubling things emerge. The first is that both sides, for all their supposed opposition, are weirdly committed to the same frame. They talk as if the only interesting question were metaphysical: is the machine really thinking, or only pretending to think? Is there intelligence in there, or only the appearance of intelligence? The skeptics answer “no” in a tone of exasperated sobriety; the enthusiasts answer “yes” with evangelical fervor. But they agree that the argument is about intelligence, as if we were all back in the freshman philosophy seminar worrying about whether other minds exist.</p><p>Meanwhile, while people shout past each other about souls and parrots, cloud providers post updated pricing tables. Input tokens: this many dollars per thousand. Output tokens: slightly more. Different models, different rates. Optional enterprise surcharge if you need guaranteed latency or data residency. Somewhere in the click-through contract, your cognition budget for the quarter is being estimated to three decimal places.</p><p>What is structurally new about these systems is not that they can generate plausible essays with poorly imitated aesthetics like this one, or simulate a better version of your coworker Kevin (if only), or make your group chat marginally funnier. We have had automatic text generators before; we have had programs that fool inattentive humans for decades. What is new is that units of prediction and generation have become billable commodities. The commodity form has crept into the interior of thinking. We are no longer just selling time, or electricity, or storage. We are selling fragments of what used to be called judgment. Although, I do acknowledge that the judgement is quite lacking. We are selling bananas with hard seeds.</p><p>You used to buy steel by the ton and labor by the hour. Now you buy sentence-completion by the thousand tokens. An LLM is many things at once—statistical engine, compressed library, uncanny parlor trick—but in its public, infrastructural role it is first and foremost a kind of meter. It converts your prompts into a stream of outputs and increments a counter. At the end of the month, the counter becomes a number on an invoice. Intelligence, whatever else it may be, has acquired a unit of account.</p><p>So the interesting question is not whether these systems are “really” intelligent, any more than the interesting question about United Fruit was whether bananas have souls. The interesting question is what happens to a world in which cognition itself shows up as a contract, a meter, and a line item. What happens when you can go long or short on predictive text, when you hedge your exposure to hallucination risk, when thinking becomes something your CFO budgets for?</p><p>--</p><p>In fairness to the technologists, they are not the first people to get overexcited about their toys and forget to look at the pipes. Long before anyone had the bright idea of charging a start-up $0.002 for every 750 characters of fake wisdom, there were other, earlier, more charming attempts to mechanise thinking. These people did not call what they were doing “AI,” because the phrase had not yet been minted. They called it cybernetics, because if you are going to reinvent the steering wheel for the twentieth century, you may as well do it in Greek.</p><p>The story usually starts with Norbert Wiener, a mathematician who spent the Second World War trying to keep anti-aircraft guns from missing their targets quite so often. His problem was simple to state and difficult to solve: a plane is not where it was a moment ago, and a shell takes time to travel; if you aim at the present, you will always be wrong. What you want, really, is a device that can take in noisy observations about a moving object, compare them against an internal model of how that object tends to move, and adjust its own behavior accordingly. Something that lives, in other words, on the thin edge between sensing and acting, constantly correcting itself as it goes. Wiener called this kind of structure “cybernetic”: systems where feedback and control, rather than mere brute force, do the work.</p><p>The crucial move was rhetorical. In <em>Cybernetics: Or Control and Communication in the Animal and the Machine</em>, Wiener argued that it made sense to talk about thermostats, anti-aircraft predictors, human nervous systems, and social organisations using the same handful of concepts: feedback, signal, noise, homeostasis, information. A heating system “knows” the temperature in the room in much the same impoverished way your body “knows” its blood sugar level. It is always slightly wrong, always adjusting. Between the thermostat and the organism there is, of course, an abyss of complexity—but on the level of diagram, they suddenly belong to the same family.</p><p>W. Ross Ashby took the family resemblance seriously. His homeostat was a sort of mechanical animal made of linked electrical circuits, each one adjusting its own parameters in response to deviations until the whole contraption settled into a stable configuration. Left to itself, it would flail through a space of possible settings until it found one that worked; kick it, and it would drift around until it found another. Ashby called this “ultrastability,” which is the kind of word only a man who spends his weekends soldering can love. What mattered was that he had built something that learned in a low, stupid way: not by storing explicit rules, but by wandering around a landscape of possibilities until it found a valley it could survive in.</p><p>Then there was Stafford Beer, who, if Netflix ever tires of true crime, deserves at least one miniseries. Beer took Wiener’s diagrams and Ashby’s regulators and applied them not to guns or gadgets but to entire firms, then to governments. His Viable System Model treated any organisation—factory, conglomerate, nation-state—as a nested set of control systems: sensors at the periphery, local regulators, higher-level “metasystems” deciding what counts as a problem, feedback loops snaking up and down. In the early 1970s, the Chilean government briefly hired him to build <em>Project Cybersyn</em>, an attempt to run the socialist economy by wire. The project famously included a control room that looked like a set from <em>Star Trek</em>, featuring swiveling white chairs and walls of screens piping in factory data in something close to real time. It did not survive the coup. But the diagrams did.</p><p>The important thing is not that any of these devices “were” intelligent in the grand, philosophical sense. The important thing is that they made it routine to talk about systems—machines, organisms, organisations—as if they performed something like thought. They sense, they compare, they adjust, they maintain patterns in the face of disturbance. They explore state spaces and settle into basins of attraction; they have, as Ashby liked to say, a certain requisite variety in their behavior to match the variety of their environment. Whether you are controlling a steam turbine or a supply chain, you are engaged in a kind of cognitive work.</p><p>What cybernetics quietly did, without asking permission from the philosophers, was to treat cognition as a property of circuits and feedback, not a mysterious fluid sloshing around inside individual skulls. The mind, in this picture, is not a ghost in a machine; it is what emerges when enough feedback loops get tangled together in the right way. Somewhere between the thermostat and the teleprinter, thinking stops being an exclusively human privilege and becomes a more general organizational achievement.</p><p>This heresy eventually leaked into the human sciences. Edwin Hutchins, in <em>Cognition in the Wild</em>, followed a team of navigators on a U.S. Navy ship and concluded that “remembering where we are” is not something that happens in any one sailor’s head. It happens in the system: charts, tools, shouted commands, procedures, rituals, instruments. The cognitive unit is not the person, but the ensemble. Cognitive psychologists began to talk about the “extended mind,” where notebooks and calculators and, later, screens and smartphones are not mere external aids but integral parts of how we think at all. Nobody called this cybernetics any more; the project had fallen out of fashion. But the idea had lodged itself quietly in the wiring: cognition as something systems do, distributed across people and devices, stitched together by feedback.</p><p>--</p><p>Once you accept that systems can think, at least in this modest, cybernetic sense, you unlock an obvious and deeply irritating question: if thinking is something systems do, who owns the system, and who gets paid? The story of the last century or so of capitalism is, in large part, the story of how we answered that question with “the usual suspects” and “definitely not you.”</p><p>Industrial capitalism, in its heroic phase, was obsessed with the body. The ideal worker was a pair of hands plus some lungs. Factories were designed around muscles and machines: how many people can you pack onto this assembly line, how many bolts can they tighten in an hour, how many loads can they haul. When Taylor and his friends came along with stopwatches and clipboards, they were still mostly measuring movement. The point was to shave a second off each gesture, to make the body act like a predictable component in a larger mechanism.</p><p>Somewhere in the middle of the twentieth century, the center of gravity shifts. The factory does not disappear, but it is joined by the office tower, the call center, the university, the software firm. There is still physical labor, but more and more people make a living by staring into rectangles, talking to each other, typing, persuading, designing, scheduling, deciding. Harry Braverman called this “the degradation of work”: management applies the same Taylorist logic to white-collar jobs, breaking down tasks, scripting interactions, embedding control into machines. The spreadsheet, the CRM system, the ticketing queue all arrive on the scene as friendly administrative tools; they are also instruments for parceling out thought.</p><p>Shoshana Zuboff, watching early computerization in the 1980s, noticed something that now seems obvious: when you add information systems to a workplace, you don’t just automate tasks, you <em>informate</em> them. Every key press becomes a record. Every call handled, every form filled, every route taken can be logged, analyzed, fed back into management decisions. The worker’s cognitive activity—what they pay attention to, how long they spend on it, how they resolve edge cases—becomes visible in a way it never was before, and therefore available to be reorganized. The system learns from them even as it disciplines them.</p><p>By the time people started talking about “cognitive capitalism,” the euphemism had settled in completely. Officially, value now comes from knowledge, creativity, innovation, relationships, attention, affect. The worker is no longer a pair of hands; they are a bundle of “competences” and “skills” and “human capital.” In practice, the question is how much of this can be routinized, captured, and resold. Customer service scripts turn emotional labor into a repeatable pattern: say the name three times, empathize in this particular register, escalate according to the flowchart. CRM dashboards turn relationships into scores and stages: lead, opportunity, pipeline, churn. Knowledge bases turn hard-won experience into documents that can be read by the next person you hire at half the salary.</p><p>Thinking, sensing, deciding, relating—these stop being the unpriced background radiation of work and become inputs in a production function, things you can, at least in theory, measure and manage. “Time to insight” becomes a metric. So does “call resolution rate,” “average handling time,” “tickets closed per engineer per week.” You can look at a graph of these numbers and convince yourself that you are watching cognition transpire.</p><p>Work was always a strange commodity, as Marx liked to point out: what you sell is not your abilities themselves, but your capacity to use them for a certain period under someone else’s direction. Labor power, rented by the hour. Cognitive capitalism adds another layer of strangeness on top. It tries to slice labor power into finer segments: not just hours, but specific cognitive processes. Deciding, empathizing, diagnosing, sorting. A call center agent reading from a script and a recommender system nudging you toward a slightly different purchase live at different ends of the same continuum. One is human cognition forced into a narrow channel by software; the other is software cognition trained on oceans of human behavior. They are cousins, raised in different sides of the same family.</p><p>--</p><p>If cybernetics told us that systems can think, and cognitive capitalism told us that thinking can be work, then the last turn of the screw comes when someone realizes that work can be prediction, and prediction can be cheap.</p><p>For a long time, prediction was something you hired people for. You paid actuaries to guess how often your policyholders would crash their cars, traders to anticipate where prices would move, inventory managers to estimate how many bananas you might sell next week. These forecasts were expensive, slow, and idiosyncratic. You could put two bright people in front of the same data and get three different answers and several ruined marriages. You hired judgment, not just numbers, and lived with the variance.</p><p>Modern machine learning, stripped of the hype, is a way of turning certain kinds of prediction into a commodity input. Give me enough historical data of the right sort, and I will give you a model that takes in new cases and spits out probabilities. Will this transaction be fraudulent? Will this ad be clicked? Will this borrower default? What word is likely to come next in this sentence? Once the model is trained, the marginal cost of one more prediction is effectively zero. Spin up some more GPUs, amortize them over the quarter, and you can forecast millions of micro-events in the time it used to take someone to sharpen their pencil.</p><p>Economists, being what they are, immediately tried to recast this in their own language. One popular formulation, from Agrawal, Gans, and Goldfarb, is that AI is best understood as a drop in the cost of prediction: wherever your process can be decomposed into prediction + judgment + action, you can now substitute machine prediction for some of the human kind. You do not eliminate humans; you rearrange them. The machine tells you which claims are likely fraudulent, and humans investigate the exceptions. The machine ranks job applicants, and humans interview the shortlist. The machine suggests a response to the angry customer, and the human hits send.</p><p>Once prediction gets cheap, you start building interfaces for it. No one wants to install TensorFlow and retrain models every Tuesday; they want a button, an endpoint, a field in a form. So you create fraud detection APIs, credit scoring services, recommendation engines as a service. You wire dashboards so that managers can drag sliders and watch risk curves update. Inside the firm, you treat model outputs as another internal resource, like server capacity or office space: something departments request and consume.</p><p>You can feel, at this point, the pieces sliding into place. Cybernetic systems that sense and adjust. Work that consists of sensing and adjusting. Prediction that automates the sensing and a good chunk of the adjusting. The language of “services,” “units,” “inputs.” We are very close now to recognition: that what we are metering and allocating and billing is not just data storage or electricity, but cognition itself, sliced thin enough to fit through the pipes.</p><p>--</p><p>At this point in the story, it would be comforting to imagine that prediction simply got cheaper in the same abstract, bloodless way that steel once did, as if the curves on an economics chalkboard had quietly shifted and no one had to get hurt. This is not how it went. Behind every neat “$0.002 per 1,000 tokens” price point is an industrial landscape that looks, once you bother to squint at it, suspiciously like a plantation. We have simply moved the crops.</p><p>If the nineteenth century had cotton and the twentieth had assembly lines, the twenty-first has data: scraped, extracted, purchased, labeled, and re-labeled with the same weary regularity that once governed harvests. First, the scraping: crawler fleets ransacking the open web, vacuuming up posts, comments, documentation, fan-fiction, newspapers, code repositories, spam, erotica, forum flame wars, recipe blogs, PDFs whose authors are now politely discovering that they have become “training data.” Then the logs: every search query, every clickstream, every “like,” every error report, every half-typed status update quietly captured as telemetry. Then the marketplaces: data brokers hawking your purchase histories and location trails in bulk, licensing arrangements in which decades of writing are repackaged as “corpora.” And finally, the bespoke crops: gig workers on annotation platforms, sweating not under the sun but under content moderation guidelines, being paid a few dollars to decide whether a sentence is hateful, helpful, sexual, suicidal, or merely boring.</p><p>The whole thing has the structure of agriculture, if you are willing to accept that the fields are now crowdsourced and the soil is sincere (and likely painful) human experience. Someone, somewhere, designs a “task”—identify objects in these images, rank these completions, rate the politeness of these responses—and ships it off onto a platform whose job is to subdivide it into bite-sized parcels and route them to anonymous workers. The tasks bloom into judgment, and the judgments are harvested as labels. Good crops are accurate, consistent, abundant; bad crops are noisy, biased, contaminated. Either way, they are fed into the machinery: they become the nutrient slurry in which the models grow. Kate Crawford calls this an “atlas,” but it is also a map of extraction: mines, dams, cables, warehouses, and, always, people.</p><p>Then comes the folding.</p><p>Training, in the fairy tale the industry tells about itself, is the moment when clever algorithms “learn patterns” from data. Training, in reality, is closer to a geological event. You take trillions of tokens—sentences, code snippets, fragments of dialogue, ratings, refusals, corrections—and you compress them into a single parametric object, a glacier made of numbers. Every word you have ever written online, every forum fight, every product review, every patient Stack Overflow answer, every quietly desperate blog post about your breakup becomes, in the aggregate, a set of weights: adjustments in the couplings between layers of an artificial neural network. During training, the model trawls through this mass, making bad guesses, being corrected, nudged, punished, rewarded, over and over, until its internal landscape settles into a shape that reliably turns “inputs” into “outputs.”</p><p>At the end of this process, you have something that is not a database, not a library, not an index, not a human mind, but something like a fossil of past cognition. Pressed into this high-dimensional stone are the grooves of our language: which words tend to follow which, which metaphors cluster with which anxieties, which arguments recur, which clichés refuse to die, how desire sounds when it is trying to be subtle, how spite sounds when it has given up on subtlety. It is not that the model “thinks” in any familiar sense, but that it carries around, in its strange internal geometry, the compacted history of millions of other people thinking in public.</p><p>From the outside, what matters is less the precise architecture—transformers, attention heads, positional encodings—than the fact that the resulting object can perform discretionary cognitive operations. You give it a prompt—an email, a symptom list, a half-finished function, a legal clause, a confession, a joke setup—and it produces a context-sensitive continuation that is not fixed in advance, not simply retrieved, but sampled from a distribution it has painfully learned to approximate. It classifies, summarizes, translates, hypothesizes, bullshits, negotiates. It can do so badly or well, cheaply or expensively, safely or disastrously, but it can do it on demand.</p><p><strong>We have, in other words, built cognition machines: technical subsystems that carry around condensed histories of thinking and can be invoked like any other industrial process.</strong> They are not free-floating minds. They are components in larger systems—the app, the organization, the platform, the state—that plug them into workflows and institutions. But within those systems, they execute what used to be recognizably human forms of work: writing a paragraph, drafting a spec, reviewing a contract, brainstorming a campaign, pretending to be your therapist.</p><p>If you want to keep the commodity thread visible, it helps to run the old images side by side. The grain elevator takes in particular kernels and outputs No. 2 Yellow Corn. The banana network takes in particular plants and outputs “Forty tons, green, Class B.” The training pipeline takes in particular sentences, particular judgments, particular lives, and outputs an abstract, tradable capacity called “tokens of prediction.” Once inside the model, no one cares which blog post contributed which eigenvector. The specifics have been scrubbed away. What remains is a standardized resource: a capacity to produce plausible continuations of almost anything.</p><p>--</p><p>Of course, you never meet the cognition machine directly. You meet a menu.</p><p>If you are a developer, you see it as an API documentation page: base URL, endpoint paths, required headers, JSON schemas. There is a cheerful example of a curl command, some nonsense about “Hello, world,” a list of error codes you will eventually memorize: 401 unauthorized, 429 too many requests, 500 internal server error. Somewhere near the bottom is the part that actually matters: pricing. $X per 1,000 input tokens. $Y per 1,000 output tokens. Optional fees for priority access, enterprise features, fine-tuning, on-premise deployment. If you are lucky, the billing unit is aligned with your mental model of usage; if you are not, you learn by watching your invoice explode.</p><p>Behind the API page, there are dashboards. They are the new ledgers: graphs of “tokens used,” “requests per minute,” “latency,” “cache hit ratio,” “peak concurrency,” “error rate.” Finance teams log in to estimate “token budgets” for the quarter, just as their predecessors once estimated travel budgets or electricity consumption. Product managers argue over whether a given feature “really needs” to call the model in the loop, or whether they can get away with a cheaper, dumber alternative. Someone in procurement starts asking pointed questions about vendor lock-in.</p><p>Wrapped tightly around all of this are the rules. Terms of service stipulating acceptable uses, content policies that ban certain categories of prompt and response, “safety layers” and moderation APIs that sit between your request and the machine’s actual output. These are written partly in human language and partly, in the manner of Lawrence Lessig’s “code is law,” directly into the protocol: rate limits, output filters, refusal templates, logging requirements. You do not just buy cognition; you buy cognition <em>under a jurisdiction</em>. The machine’s thoughts are pre-emptively fenced.</p><p>At the center of this interface economy sits one of the most boring objects imaginable: the token.</p><p>Technically, a token is a subword unit, a fragment of text produced by some algorithm that tries to balance efficiency and reuse. “Banana” might be one token; “internationalization” might be split into several. From the machine’s point of view, tokens are the atoms of its universe: everything it sees and produces arrives as sequences of these pseudo-words. From an engineer’s point of view, tokens are how you measure throughput and capacity: how long a prompt is, how much memory it will consume, how many steps you will have to take.</p><p>Economically, the token is the thing you pay for. Billing systems do not care about your categories—“draft,” “conversation,” “experiment,” “crisis”—they care about the count. $X per 1,000 in, $Y per 1,000 out. Use more; pay more. Negotiate a discount if your volumes are high enough. Monitor usage; clamp down on departments whose “exploration” is getting out of hand. The unit of thought has a unit price.</p><p>Managerially, tokens become a way of thinking about thinking. Well-meaning executives ask how many tokens their organization is consuming per quarter and whether they are “getting value.” Internal metrics appear: tokens per ticket resolved, tokens per feature shipped, tokens per customer touched. Utilization dashboards track which teams are “underusing” the models and which are “over-indexed.” Somewhere in a slide deck, a bullet point solemnly compares “tokens per revenue dollar” across business lines.</p><p>If the grain trade needed the bushel and timber needed the board-foot, cognition machines need the token. Without a standardized, countable grain size for language, you cannot meter cognitive work, cannot exchange it, cannot insure it, cannot fold it into contracts. A model that speaks only in paragraphs is a curiosity; a model that speaks in tokens is an asset.</p><p>What is being traded on these APIs is not primarily storage or CPU time. Those are there, of course, humming dutifully in the background, eating electricity. But the surface that users interact with, the surface that gets priced, is discretionary cognitive operations, sliced into tokens. You are not buying a server; you are renting a small, temporally bounded participation in the fossilized thought of others.</p><p>--</p><p>At this point, we should probably admit that we are deep into a confusion of names. We talk loosely about “AI,” “models,” “services,” “tools,” “platforms,” as if they were interchangeable. We collapse whole sociotechnical arrangements into brand names. We pretend that the thing we log into and the thing that chews through gradients are the same. It might be useful, in the old Chinese imperial sense, to pursue a rectification of names: to ask what these creatures actually are, and what they are doing, and what we are doing with them.</p><p>Start with the widest circle:</p><p>A cognitive system is any configuration of humans, machines, and institutions that maintains and transforms patterns of information via feedback to get things done under uncertainty. The bridge crew of a ship triangulating their position off buoys and radar echoes is a cognitive system. So is a hospital triage ward juggling patient flow and resource constraints, a trading desk where algorithms flag opportunities and humans decide which ones to act on, a content moderation team filtering flagged posts with the aid of dashboards and heuristics. The unit of analysis is not the solitary genius but the ensemble of sensing, remembering, deciding, and acting, held together by protocols and practices.</p><p>Within these systems, not all thinking is created equal. Some of it is ad hoc, unpriced, done for love or fear or duty. Some of it is structured as work. Some of that work, in turn, takes on a particular form: it becomes something you can allocate, schedule, trace, and eventually sell. When that happens, you are edging into the territory of what we might call a cognition market.</p><p>A cognition market is any arrangement where cognitive work—prediction, classification, generation, evaluation—is exposed as a repeatable service, allocated among users, and bounded or priced in some unit. Historically, this might look like consulting hours billed at $400 an hour, or paralegal time charged to a client, or piecework annotation tasks paid at a few cents per label. In its more contemporary forms, it looks like annotation platforms where human judgment is fungible; AI APIs billed per call or per token; internal chargeback systems where one division of a company pays another for “model usage.” The details differ, but the structure is the same: cognition pulled out of its context, packaged, assigned a metric, and made mobile.</p><p>Now we can name the thing squatting at the intersection of these two domains, the object we have been circling all along:</p><p>A cognition machine is a technical subsystem in a broader cognitive system that has been configured to ingest data at scale, fold it into durable internal structures (weights, rules, memories), and expose its discretionary cognitive operations—prediction, classification, generation, evaluation—through standardized interfaces whose use is metered and allocated in discrete units (tokens, calls, capacity quotas). Emerging from the convergence of cybernetic control systems, distributed cognition, and cognitive capitalism, cognition machines epitomize the contemporary regime in which the faculties of thought themselves become programmable, composable, and tradeable; LLMs, trained on data farmed from planetary-scale human activity and sold per token, are its clearest current instantiation.</p><p>The point is not that cognition machines replace cognitive systems. They do not. They live inside them, as organs live inside bodies, as elevators live inside buildings. Nor do they replace cognition markets; they make those markets finer-grained, more automated, more infrastructural. A cognition machine is what happens when you take a slice of a cognitive system—some segment of its sensing, inferring, or expressing capacity—and force it into commodity form. It must be standardized enough to expose an interface; stable enough to be relied on; and quantized enough to be metered.</p><p>If you like diagrams, you can imagine three overlapping circles: systems, markets, machines. The mentally clean zones are where nothing especially interesting happens: cognition with no markets (a family deciding what to have for dinner), markets with no specialized machines (a neighbor giving you advice for free), machines with no markets (a hobbyist’s homemade chatbot). The nausea sets in where all three overlap, and there you find: the model endpoint, the usage dashboard, the invoice.</p><p>--</p><p>Once you have named the cognition machine, it starts showing up everywhere, like a new car model you regret buying. The category is less a single beast than a small, vicious zoo. It is worth taking a brief, hopelessly incomplete tour, if only to see how many ways thought can be diced into parts and sold back to itself.</p><p>First, the obvious ones: inference engines as a service. These are the public-facing creatures: large language models, image generators, vision APIs, risk-scoring services. Their business model is simple enough to fit into a startup pitch: you send tokens in, you get predictions or generations out, you pay per unit. The cognition machine is front and center, and the commodity form is naked. Every prompt and response leaves a trace in a billing log. If you squint past the logos, each of these services is a narrow funnel into a giant folded model, a coin-operated slot on the side of an invisible brain.</p><p>Then there are the organizational control machines, the internal cybernetic systems that never see daylight. Think of logistics optimizers arranging delivery routes in real time, warehouse management systems assigning pick orders to human workers and robots, operations dashboards that pull live feeds from sensors and spit out recommendations. In these cases, the cognition machine is not a public endpoint but a component inside enterprise software. The commodity form is partly internalized: the “unit of thought” might be bundled into a per-seat license, an on-premise appliance, an internal chargeback rate for “optimization runs.” The model’s predictions are not sold retail; they are baked into the price of “operational excellence.”</p><p>Next, the platform orchestrators, creatures whose natural habitat is the feed and the ad auction. News recommender systems, social media ranking algorithms, music and video suggestion engines, search result rankers. Here, the primary commodity is shaped attention, priced in CPMs and conversion rates. But underneath the glossy dashboards and advertising metrics are cognition machines doing relevance scoring, similarity calculations, user segmentation, auction clearing. A recommendation engine that guesses what you might click next is, structurally, an inference service; it is just that the “unit” being priced is not the prediction itself, but the downstream behavior it nudges into existence. You never see the tokens; you only see the impulse.</p><p>Fourth, the data farm machines, those quiet, exploited cousins we met earlier on the plantations. These are the annotation tools, the RLHF platforms, the “human in the loop” pipelines built to structure and capture human judgment at scale. They are cognition machines in a more perverse sense: they do not themselves perform the interesting cognition, but they orchestrate its extraction, regiment it into task templates, and deliver it as labeled data for other models. Here, the commodity is raw mental labor; the unit might be “annotations per hour” or “judgments per thousand items.” The machine is a loom for weaving other people’s thinking into training sets.</p><p>Finally, there are the protocol-embedded cognition machines, which hide inside the rules of games we pretend are neutral. Risk oracles that feed scores into lending protocols; compliance modules that automatically flag transactions for extra scrutiny; safety filters that sit in front of public interfaces and veto certain outputs. Their outputs trigger or constrain other systems: a “yes” or “no” here makes money move or freeze over there. In these cases, thought is not just a service but a condition: a requirement baked into contracts, regulations, platform policies. The unit might be a binary decision, a risk band, a compliance label. The price is paid in transaction fees, legal risk, slowed throughput.</p><p>In each of these types, the commodity aspect hides in a slightly different place. Sometimes it is the API bill. Sometimes it is the SaaS contract. Sometimes it is the hourly wage of a labeler, the bid in an ad auction, the basis point shaved off a transaction. But in each case, you can draw the same diagram: somewhere, a system is taking in data, transforming it into a standardized cognitive operation, and handing that operation off under conditions that make it legible to markets.</p><p>--</p><p>At long last, naming the thing allows us to choose to become either apocalyptic or bored. The apocalyptic response is to declare that we have finally mechanized thought and therefore doomed ourselves; the bored response is to insist that this is just capitalism doing capitalism, nothing to see here. Both are evasions. The more honest position is discomfort: to admit that we are sliding into a world where thinking is something you can budget for, depend on, and default on, and that this brings with it a set of tensions that are neither unprecedented nor entirely familiar.</p><p>The first is dependence versus autonomy. The more organizations lean on cognition machines for everyday judgment—triaging tickets, drafting responses, summarizing reports, flagging anomalies—the more they entangle their own capacity to act with someone else’s infrastructure. It is one thing to rely on cloud storage; it is another to rely on stranger reasoning. If access is priced aggressively, revoked under geopolitical pressure, or constrained by shifting content policies, whole domains of practical intelligence may flicker in and out of reach. “Cognitive dependency risk” becomes as real as counterparty risk, but harder to model: what happens to a bureaucracy that has forgotten how to think in the absence of autocomplete?</p><p>Second, opacity versus bargaining power. Ordinary commodities usually come with specifications and grades; you can read the label on the bag of grain and compare it to the futures contract. Cognition machines come mostly as black boxes, protected by trade secrets and the genuine difficulty of making their internal operations intelligible. You can measure outputs—accuracy on benchmarks, latency, cost per token—but you cannot easily see how those outputs are produced, or how the price you are paying relates to the actual cost. This asymmetry weakens the bargaining position of users, regulators, and even downstream developers. It is hard to renegotiate a contract when you do not know what you are really buying.</p><p>Third, standardization versus pluralism. Commodities impose standards; that is their whole point. No. 2 Yellow Corn is interchangeable across farms and regions. No. 2 Spring Wheat does not care about local varietals. When cognition is commoditized, similar pressures apply. Models trained on globalized, platform-shaped data may gradually flatten language, argument styles, even moral reasoning into what is easiest to model and safest to sell. The same few cognition machines, exposed through the same endpoints, risk becoming a kind of global cognitive infrastructure. Over time, anyone who opts out may find themselves speaking a dialect that the rest of the world, trained on the standardized version, hears as noise.</p><p>Fourth, extraction versus participation. Data plantations and RLHF farms externalize the messy parts of cognition to precarious labor and unpaid publics. The commodity form hides these relations of production under the smooth surface of the API. You pay for tokens; you do not see the people labeling trauma content in Nairobi for pennies, or the writers whose unpaid work has been liquefied into gradients. One can imagine a more humane design, in which the people whose cognition is being folded into the machines are treated not as exhaust or raw material but as co-authors, co-owners, or at least parties to an honest negotiation. But that would require acknowledging that cognition machines are social products, not just technical marvels.</p><p>Finally, control versus drift. The more we embed cognition machines into protocols, contracts, and infrastructures, the more their biases, blind spots, and affordances will shape what kinds of decisions are easy and what kinds are unthinkable. Any system that makes certain kinds of thoughts cheaper will tend to produce more of them. A world saturated with cheap prediction may discount forms of reasoning that cannot be easily decomposed into prediction + judgment + action, or that resist being tokenized. The risk is not that we will all be replaced by machines, but that we will gradually reorganize our institutions and desires around what machines can conveniently do.</p><p>None of this demands an immediate policy program or manifesto. It does, however, demand that once you see cognition as a commodity, certain questions can no longer be politely ignored. Who owns the machines? Who supplies the data? Who sets the prices? Who can refuse? What forms of thinking are being amplified, and which are being quietly starved?</p><p>--</p><p>If you want a closing image, you could do worse than the archive.</p><p>Imagine, a few decades from now, some exhausted PhD student sitting not in a dusty basement with shipping manifests from the Great White Fleet, but in a corporate data room with access to CSV exports labeled token_usage_2024_2032_final_v7.csv. Row after row: timestamp, endpoint, customer ID, model version, tokens in, tokens out, dollars billed. Our descendant scrolls and scrolls, trying to reconstruct how we lived.</p><p>They will see the spikes: product launches, marketing campaigns, moderation panics, election seasons, crisis response. They will see which product teams burned the most tokens, which departments stayed stubbornly analog. They will see the moments when regulators forced in new safety constraints and whole categories of prompt quietly started returning refusals. They will see the rise of new entrants, the consolidation into a few dominant providers, the slow creep of price changes. Perhaps they will be able to correlate token usage with layoffs, or with political events, or with shifts in public language.</p><p>To them, these logs will look very much like the grain ledgers of the nineteenth century: traces of a moment when something once particular and lived—fields, families, ways of talking, styles of attention—was turned into a standardized unit and made to flow through pipes. Where Kepner and Soothill puzzled over how a fruit became an empire, our future historian will puzzle over how thought became a line item.</p><p>Maybe cognition machines are our version of the grain elevator and the steam engine and the Chicago pit, all rolled into one: careful, boring apparatuses that end up reshaping what a mind is allowed to be. Thoughts, like kernels, disappear into chutes and come out the other side standardized and ready for trade. We can still think outside them, of course. People will still have ideas in the shower, still brood on walks, still write unprompted poems no one reads. But more and more of the world will be organized by the thoughts we buy in bulk: the templates, the drafts, the risk scores, the recommendations, the summaries.</p><p>The question, in the end, is not whether these machines “really think.” That question will age about as gracefully as asking whether the telegraph really “speaks.” The more interesting question is what happens to us when “thinking” is something you can go long or short on, hedge, arbitrage, securitize, and default on.</p>