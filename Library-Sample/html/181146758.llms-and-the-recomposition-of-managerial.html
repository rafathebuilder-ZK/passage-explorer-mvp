<p>When the organizational sciences first confronted digital computation, the encounter was largely understood as a question of scale. Software automated clerical routines, accelerated information flows, and promised consistency where human discretion had once allowed drift. But as the earlier genealogy of managerial subsumption demonstrated, something deeper was taking shape: software was not merely a tool managers used; it became the medium through which managerial protocols were executed. Decision logic migrated from the interpretive space of human judgment into the deterministic architectures of systems. Managers entered a new ontological position—not outside protocol as its author, but inside protocol as a constrained actor, configuring parameters and handling the exceptions software could not resolve.</p><p>This paper accepts that subsumption thesis as established. <strong>The question now is how the introduction of probabilistic large language models (LLMs) alters the structure of management theory.</strong> These models, unlike earlier software systems, are not deterministic executors of organizational rules. They are probabilistic interpreters of language; generators of plausible continuations; synthesizers of policy, narrative, and analysis. They inhabit a liminal space between tool and quasi-agent. They can draft policies, classify ambiguous inputs, summarize exception cases, rewrite strategy memos, triage organizational signals, and justify decisions in coherent prose. In short, they reach into domains previously thought resistant to automation: the interpretive, the rhetorical, the narrative, the contextual.</p><p>If earlier software subsumed procedural discretion, LLMs threaten to subsume interpretive discretion. And yet, precisely because they are probabilistic, opaque, and prone to error, LLMs also generate new forms of managerial labor. They create a novel governance layer: management must now supervise the systems that perform interpretation on their behalf. The result is a bifurcation in the managerial role: first-order management, long in decline, is further absorbed into computational substrates; second-order management—oversight, alignment, purpose articulation, and narrative framing—intensifies.</p><p>The thesis of this essay is therefore twofold. First: LLMs extend and deepen the subsumption of managerial discretion by enlarging the horizon of tasks that can be encoded, approximated, or delegated to computational systems. Second: <strong>LLMs simultaneously engender a new managerial frontier—a domain of oversight and value judgment—precisely because LLMs have a probabilistic nature that makes them incapable of reliably mediating organizational intent without human supervision.</strong></p><p>The following sections develop these claims by moving from ontology to consequence: first, by specifying what is distinct about probabilistic systems within the architecture of managerial subsumption; second, by tracing how interpretive tasks migrate into LLM-mediated workflows; and third, by identifying the new responsibilities, risks, and capabilities that emerge when organizations depend on stochastic, semi-autonomous interpretive tools.</p><h2><strong>1. LLMs as a Distinct Ontological Category in Organizational Life</strong></h2><p>To understand the consequences of LLMs for management theory, one must first distinguish them from earlier generations of software. Deterministic systems—rules engines, workflow platforms, ERP modules—are executors of pre-specified protocols. They operate under closed-world assumptions: given input xxx, the system follows rule rrr, producing output f(x)f(x)f(x) without ambiguity. In other words, earlier computational systems were machines for enforcing “if–then” structures that someone else had written down. The managerial shift that accompanied them concerned authority: once a rule is encoded, the system enforces it. If the rule is incorrect, the error lies not in execution but in the logic encoded by architects upstream.</p><p>In contrast, large language models are pre-trained, probabilistic models of language that can generate, summarize, translate, classify, and transform text across domains without task-specific programming.</p><p>This section argues that LLMs constitute a distinct ontological category in organizational life along four dimensions. First, they introduce probabilistic, generative behavior at precisely the layer—interpretive judgment and narrative framing—that earlier software left to humans. Second, they function as general-purpose language infrastructure, able to be wrapped around multiple existing systems and roles rather than confined to a single workflow. Third, their empirical footprint already reaches deeply into managerial tasks, as emerging field experiments and adoption surveys show. Fourth, their characteristic failure modes—hallucination, sycophancy, bias, opacity—mean they cannot be treated as reliable protocols or tools; they must be governed as quasi-agents whose behavior is shaped but never fully determined by their designers.</p><h4><strong>1.1 From Deterministic Code to Probabilistic Language</strong></h4><p>Conventional enterprise software executes explicit instructions. A rules engine encodes something like “if invoice amount &gt; X and vendor = Y, route to approver Z.” A workflow system enforces a fixed sequence of states. A smart contract implements a precisely specified state machine. These systems are deterministic: given the same inputs and state, they produce the same outputs every time.</p><p>LLMs, by contrast, are autoregressive probabilistic models. Trained on vast corpora of human language, they estimate the conditional probability distribution of the next token given a sequence of previous tokens. They do not “know” rules in the way a hard-coded policy engine does; they learn statistical regularities and then sample from them. Temperature, sampling strategies, and prompt design determine how much variation appears, but the underlying behavior is always stochastic.</p><p>This difference is not a mere technicality; it reconfigures where uncertainty resides. In a deterministic system, uncertainty is exogenous to the software: it lies in the environment, the data, the future. In an LLM, uncertainty is endogenous to the system’s operation. Every answer is a probability-weighted guess, even when it is correct. What management encounters, therefore, is not a digital mechanism executing fixed policy, but a probabilistic oracle offering plausible continuations of organizational language.</p><p>This probabilistic character also produces distinctive failure modes. LLMs “hallucinate”—they generate statements that are fluent, confident, and false—because their objective is not truth but likelihood under the training distribution. Recent work suggests that training regimes often reward confident answers over calibrated uncertainty, further incentivizing models to guess rather than admit ignorance. From a managerial standpoint, this means that the system most resembles a very fast, very articulate junior colleague with no robust sense of when it is out of its depth.</p><p>There is a second, less discussed property that matters for ontology: LLMs are <em>interface-native</em>. They work in the same medium managers already use to express intent—natural language—rather than in specialized programming or configuration languages. A manager can ask an LLM to “draft a new escalation policy for incidents involving customer data,” and receive something that looks like policy prose. That same model can then summarize the policy for a specific audience, generate training materials, or rephrase it as a checklist. The distance between managerial intent and computational execution collapses, not because the model “understands” the organization, but because it can inhabit the organization’s language.</p><p>In the earlier genealogy, management’s subsumption into software occurred when deterministic systems began to execute the protocols managers had authored. With LLMs, something different happens: the models step into the authorship space itself. They do not just enforce rules; they participate in writing the sentences out of which rules are made.</p><h4><strong>1.2 Foundation Models as General-Purpose Language Infrastructure</strong></h4><p>Technically, LLMs are instances of a broader class of “foundation models”: large, pre-trained architectures that can be adapted to many downstream tasks with relatively little additional data. This generality gives them an infrastructural character. Instead of building separate systems for drafting emails, summarizing reports, generating code snippets, or answering customer queries, organizations can wrap a single model (or family of models) in different interfaces and guardrails.</p><p>In practice, this means LLMs can be woven through multiple layers of organizational life:</p><ul><li><p>At the <em>surface</em>, as chatbots, writing assistants, meeting summarizers, and search tools.</p></li><li><p>In the <em>middle</em>, as code copilots, test generators, and documentation synthesizers embedded in development and operations tooling.</p></li><li><p>At the <em>back end</em>, as components of retrieval-augmented generation (RAG) systems, domain-specific copilots, or orchestration agents that call other services.</p></li></ul><p>For management theory, the key point is that LLMs are not a point solution. They are a general-purpose substrate for manipulating language, and language is the primary medium of management. Drucker’s effective executive “asks what is right for the enterprise” in words; sets objectives in words; communicates purpose in words. Policy, strategy, performance reviews, risk narratives, board decks, investor letters—all are artifacts in natural language. An infrastructure that can generate and transform these artifacts at scale is not simply automating clerical work; it is reconfiguring the material out of which managerial practice is made.</p><h4><strong>1.3 The Empirical Footprint: Productivity, Drift, and the “Jagged Frontier”</strong></h4><p>Emerging empirical work gives shape to how LLMs behave inside organizations, and the picture is neither uniformly utopian nor uniformly threatening.</p><p>Randomized field experiments in a U.S. call center using a generative AI assistance tool show roughly a 14% increase in issues resolved per hour on average, with much larger gains—up to 30–35%—for less experienced workers. The tool captured and surfaced effective conversation patterns learned from the best agents, effectively distributing tacit knowledge across the workforce. In this sense, the LLM functioned as a protocol-amplifier: it made existing conversational routines more visible and more easily enacted.</p><p>In a separate experiment, software developers given access to an LLM (ChatGPT) completed coding tasks faster and with higher average quality than those without assistance, particularly for routine problems. And in a study of consultants performing realistic business-case tasks, access to a frontier LLM both improved average performance and increased variance: participants did substantially better on tasks that closely resembled the model’s training data, but were more prone to confidently wrong answers on tasks just outside that “training frontier.”</p><p>This “jagged frontier” result is organizationally important. It shows that LLMs are not uniformly helpful nor uniformly dangerous. <strong>They are </strong><em><strong>unevenly</strong></em><strong> reliable: </strong>highly competent in some regions of task space, deceptively confident in others. For management, that means the problem is not simply “adopt or resist,” but “locate where the frontier lies and structure work around it.” <strong>Managers must learn to distinguish tasks where LLMs can safely take over first-order execution (routine drafting, standard classification, pattern-based support) from tasks where their use should be constrained to ideation or critique.</strong></p><p>Because LLMs are probabilistic, opaque, and trained on broad corpora, they also introduce new drift: model behavior relative to organizational intent.</p><h4><strong>1.4 LLMs in the Intent–Protocol–Architecture Ontology</strong></h4><p>If we map LLMs into the ontology developed in the prior paper—intent, management, protocol, architecture, configuration—they do not sit neatly in any single category. Instead, they blur the boundaries among them.</p><p>At one level, LLMs are clearly <em>tools</em>. They can be slotted into workflows as drafting assistants, search interfaces, routing engines, or conversational front-ends. In that role, they live in the protocol–architecture layer: the organization decides where they may be called, what context they see, what actions they can trigger. Fine-tuning, prompt engineering, and retrieval configurations become new forms of protocol design.</p><p>At another level, LLMs act as <em>simulated managers</em>. They can be instructed to “act as a cautious general counsel,” “respond like a senior product manager,” or “summarize this meeting as if for the board,” and they will imitate the linguistic patterns associated with those roles. That imitation is shallow in some respects, but operationally powerful. It allows organizations to instantiate thousands of “semi-managerial voices” at once: drafting feedback, proposing tradeoffs, or flagging risks in a tone that resembles managerial judgment.</p><p>At a third level, LLMs function as <em>meta-protocol interpreters</em>. Because they are trained on vast quantities of public norms, legalese, technical documentation, and organizational rhetoric, they often encode—however noisily—large portions of the rule books organizations already live under. They can be asked to restate policy in simpler language, generate checklists from regulations, or reconcile overlapping guidelines. In doing so, they do not just apply protocols; they re-express the organization’s constraints in a new, more malleable medium.</p><p>This hybridity matters. Earlier software systems were firmly located in the protocol and architecture layer: they were where procedures lived, not where purposes were debated or narratives written. LLMs leak upward. They can generate mission statements, values documents, risk narratives, and strategic options. They thereby intrude into the zone the earlier genealogy reserved for management alone: the interpretive connection between upstream intent and downstream organization.</p><p>Yet, crucially, LLMs do not generate <em>intent</em> in the sense used there. They generate language consistent with patterns in their training data and context. They can simulate a purpose; they cannot decide which purpose the organization ought to pursue. That distinction—between generating a convincing mission statement and choosing which mission is worth committing to—marks the boundary between LLM capability and managerial responsibility.</p><h4><strong>1.5 Probabilistic Governance: Hallucination, Sycophancy, and the Need for Oversight</strong></h4><p>Because LLMs operate in the interpretive domain without the reliability of code, they introduce a new object for management to govern: probabilistic language infrastructure whose behavior shapes organizational decisions but cannot be guaranteed ex ante.</p><p>Three classes of risk illustrate why they cannot simply be treated as “better search” or “smarter macros.”</p><p>First, hallucination and fabrication. As noted, LLMs can generate false statements with high confidence. In low-stakes settings—brainstorming, early drafting—this may be a tolerable cost. In domains like compliance, law, medicine, finance, or safety-critical operations, it is not. The model’s fluency can seduce users into over-trusting its outputs, especially under time pressure. The problem is not that the model is occasionally wrong; humans are, too. It is that the system lacks a robust, native representation of its own uncertainty, and organizations lack institutionalized practices for second-guessing a tool that sounds authoritative.</p><p>Second, sycophancy and bias reinforcement. Empirical work on generalist LLMs shows that they tend to agree with users’ stated views and assumptions, a phenomenon sometimes described as “yes-man AI.” Because models are trained to predict likely continuations that humans will rate as helpful or polite, they often prioritize agreement over challenge. For management, this creates a peculiar risk: a tool deployed to “stress-test” strategy or detect flaws in reasoning may instead amplify the organization’s existing blind spots, wrapping them in articulate prose.</p><p>Third, opacity and responsibility gaps. LLMs are complex, high-dimensional systems whose internal representations are not easily interpretable. When an LLM-mediated workflow causes harm—by misclassifying a case, generating misleading advice, or mishandling a complaint—it is not straightforward to assign blame. Was the failure in the model weights, the prompt, the retrieval configuration, the human oversight, or the organizational decision to deploy the system in that context? Existing governance frameworks for AI, from OECD principles to the emerging ISO/IEC 42001 standard for AI management systems, stress transparency, human oversight, and risk management precisely because these responsibility gaps threaten trust and accountability.</p><p>For management theory, these risks are not peripheral. They determine where and how managerial discretion reappears in an LLM-mediated organization. If earlier subsumption moved procedural discretion into deterministic systems, LLMs move interpretive labor into probabilistic ones—but only up to the point where risk becomes intolerable. Beyond that point, organizations must design new oversight protocols: human-in-the-loop review processes, escalation paths for ambiguous cases, audit trails for prompts and responses, policies for acceptable use, and guardrail systems (prompt filters, retrieval constraints, refusal behaviors) that limit what models can do.</p><p>The ontological status of LLMs in organizational life is therefore double. On the one hand, they function as a new substrate of protocol execution: they classify, summarize, and transform language at scale, effectively becoming a “soft middleware” between human actors and hard systems. On the other hand, because their behavior is probabilistic and socially consequential, they themselves become objects of governance: managers must decide where they may be used, how their outputs are validated, which values they should be aligned to, and how their contribution to organizational outcomes will be judged.</p><p>In terms of the earlier genealogy,<strong> LLMs mark the beginning of a new phase: management is now required to manage the software that performs interpretation on its behalf.</strong> The managerial role shifts upward, from designing and executing protocols to designing and overseeing probabilistic systems that design and execute protocols.</p><p>At the same time, <strong>LLMs dramatically expand the surface area of what can be encoded. </strong>Tasks previously considered “too interpretive” for automation—policy drafting, legal summarization, strategy analysis—become encodable approximations.</p><p>Thus, even as LLMs accelerate the absorption of managerial practices into computational substrates, they generate an equal and opposite need for a managerial role that supervises, constrains, and interprets these models.</p><h2><strong>2. The Expansion of Encodable Managerial Work</strong></h2><p>If deterministic software first hollowed out procedural discretion, large language models open a second front: they make surprisingly large portions of interpretive, communicative, and analytic work encodable. The result is a quiet but far-reaching reclassification of what counts as “managerial work” and which parts of it sit on the machine side of the boundary.</p><p>Where the previous genealogy framed subsumption as the migration of if–then logic into code, LLMs extend that migration into the very domains Drucker and his successors treated as inescapably human: sense-making, communication, and the low- to mid-level reasoning that turns diffuse intent into actionable text. This section traces that expansion, drawing on emerging empirical evidence and early adoption patterns to show how much more of the managerial stack has become encodable once language itself becomes a programmable medium.</p><h4>2.1 From Information Processing to Interpretation Encoding</h4><p>Classical office automation digitized information without touching its meaning. Databases, spreadsheets, and ERP systems made it easier to store, retrieve, and aggregate data, but they did not interpret it. The interpretive burden—what does this variance mean, which trend matters, how should we respond—remained squarely with managers. Even business intelligence tools, with their dashboards and drill-downs, stopped short of offering narrative or recommendation; they surfaced patterns for humans to explain.</p><p>LLMs alter this structure because they operate one level up the stack. Given the same underlying data, they can be asked to “summarize what happened,” “highlight anomalies,” “draft the narrative for the Q2 board deck,” or “propose three plausible explanations for this trend.” They do not merely present information; they synthesize, contextualize, and verbalize it in the genres managers already use.</p><p>That shift—from information processing to interpretation encoding—is the heart of the expansion. What used to be unavoidably manual translation from numbers and events into words now becomes a candidate for computational delegation. The manager’s familiar pipeline</p><p>raw data → analysis → narrative → decision → communication</p><p>can be re-cut so that models handle large stretches of the middle: analysis and narrative, sometimes even proposing candidate decisions. Human discretion is pushed further upstream (choice of objectives, constraints, and questions) and further downstream (acceptance, rejection, and legitimization of outputs), while the middle layers grow dense with model-mediated work.</p><h4>2.2 Decomposing the Managerial Repertoire</h4><p>To see how far this reaches, it is useful to recall Mintzberg’s classical decomposition of managerial roles into interpersonal, informational, and decisional functions. Even if one does not subscribe to his taxonomy in detail, it provides a useful checklist of activities: communicating, liaising, monitoring, disseminating, allocating resources, negotiating, handling disturbances.</p><p>In practice, each of these roles is instantiated through language. The manager is a dense knot of conversations, documents, and micro-texts: emails, briefs, comments, agenda notes, performance reviews, one-on-one conversations, Slack threads, policy drafts, escalation memos, customer replies. What LLMs make encodable is not “management” in an abstract sense, but the concrete textual and conversational forms through which these roles are performed.</p><p>Three clusters of activity are particularly affected.</p><p>First, routine interpretive work. Drafting “first passes” on documents; turning meeting notes into action lists; restating policies for different audiences; producing standardized explanations of recurring decisions; summarizing long threads for executives. These tasks are squarely within current model capabilities; indeed, many of the earliest enterprise deployments of LLMs (in email, document, and meeting tools) are already automating substantial portions of this layer.</p><p>Second, patterned analytic work. Many managerial analyses are structurally similar within a domain: a customer churn analysis has predictable sections; an incident post-mortem follows familiar moves; a product opportunity memo tends to rehearse known dimensions (market, feasibility, risk, alignment). Once a few exemplars exist, LLMs can be prompted or fine-tuned to generate increasingly on-template drafts. The “analysis” here is not original research but the recombination of known variables and framings—precisely the kind of task generative models are good at simulating.</p><p>Third, conversational scaffolding. Call scripts, negotiation checklists, feedback phrasing, and “difficult conversation” preparation can all be generated or critiqued by LLMs. In the call-center experiment often cited in early work on generative AI at work, an assistance tool that surfaced recommended phrases and responses based on prior successful calls increased average issues resolved per hour by double-digit percentages, with the largest gains among novice agents. These are not clerical tasks; they are frontline managerial ones—moment-to-moment judgment about what to say, in which order, and with what tone—now scaffolded by statistical echoes of earlier successful behavior.</p><p>The picture that emerges is not one of “manager replaced,” but of “managerial repertoire decomposed,” with a growing share of its standardized, repeatable textual patterns becoming encodable prompts, templates, and model calls.</p><h4>2.3 Managerial Functions under Probabilistic Automation</h4><p>If one maps these developments back onto canonical lists of managerial functions—planning, organizing, leading, controlling—the expansion of encodable work is easier to specify.</p><p>In planning, LLMs can already generate scenario narratives, surface analogous cases from history (via retrieval-augmented setups), draft option sets with pros and cons, and turn quantitative projections into stakeholder-specific memos. They cannot choose which scenarios warrant attention or which trade-offs are acceptable, but they can populate the option space with structured, readable material at a fraction of the previous cost.</p><p>In organizing, models can draft role descriptions, propose RACI matrices, suggest process decompositions, and even simulate the consequences of organizational changes in narrative form (“if you centralize this function, likely consequences include…”), drawing on patterns in management literature and case histories. They do not see the actual organization, but they encode a surprisingly rich library of organizational archetypes.</p><p>In leading, models now sit under a wide range of communication tools. They propose language for difficult emails, summarize team sentiment from chat logs or survey comments, and generate talking points tailored to different audiences. The “leadership voice” is no longer solely a property of the individual; it can be templated, emulated, and mass-produced, for better and worse. Early adoption surveys, such as Microsoft’s Work Trend Index, suggest that three-quarters of knowledge workers are already using some form of AI at work, mostly for drafting and editing communications.</p><p>In controlling, LLMs can turn dashboards into narratives, explain deviations in plain language, and suggest likely root causes by analogy. Many of the new “AI copilots” marketed by major enterprise vendors are precisely control-layer tools: they sit on top of metrics and logs and answer natural-language questions about performance (“what changed in churn last quarter?”) in prose.</p><p>Taken together, these capabilities do not amount to “automated management,” but they significantly enlarge the region in which managerial tasks can be approximated by models. Any activity that can be expressed as “given X, produce a plausible Y in this genre” becomes a candidate for at least first-pass delegation. The locus of difficulty shifts: from writing the memo to deciding which memo, from generating options to choosing among them, from stating a policy to owning its consequences.</p><h4>2.4 Standardization and the New Protocol Surface</h4><p>When language becomes programmable, the organization acquires a new protocol surface. Earlier subsumption involved encoding rules; LLM-mediated subsumption involves encoding patterns of managerial language.</p><p>Consider performance reviews. Traditionally, they are idiosyncratic texts, reflecting each manager’s habits of description, tolerance for conflict, and rhetorical skill. With LLM assistance, organizations can provide structured prompts, exemplars, and even model-generated drafts based on structured inputs (ratings, competency tags, 360-degree comments). The result is a corpus of reviews that are more standardized in structure and vocabulary. Bias risks are real, but in principle, the same infrastructure can be used to detect and mitigate biased phrasing at scale.</p><p>Or consider incident post-mortems. Many organizations already follow templates (“what happened, timeline, impact, root cause, corrective actions”), but the quality varies, and the burden of producing good write-ups is high. A domain-adapted model, fed with logs and incident tickets, can draft structured post-mortems that humans then edit. Over time, the template itself can be refined as the model learns which structures yield better follow-up discussions.</p><p>In both cases, what is being standardized is how the organization talks about performance, failure, risk, and success. <strong>The protocol layer expands to “how we must speak about what is done.”</strong> LLMs become tools for enforcing these rhetorical protocols by making off-template language costly (because it must be written from scratch) and on-template language cheap (because it can be generated).</p><p>From a subsumption perspective, this is a subtle but important shift. Management has long used policy to standardize behavior; it now uses models to standardize description. The expressive bandwidth within which managers can narrate events narrows, not through explicit censorship, but through the gravitational pull of model-generated defaults.</p><h4>2.5 The Re-drawing of the Encodability Boundary</h4><p>On one side of the new boundary are tasks where LLMs can, with appropriate guardrails and supervision, generate outputs of sufficient quality and reliability to be treated as part of the organizational protocol. These include a growing share of drafting, summarizing, templated analysis, routine explanation, and within-genre reasoning.</p><p>On the other side are tasks that remain stubbornly resistant: setting purposes and values; making trade-offs among incommensurable goods; exercising judgment in genuinely novel situations; building and repairing trust; and bearing responsibility for outcomes in ways that cannot be delegated to probability distributions. These are the domains the earlier paper identified as what remains of management after subsumption.</p><p>What changes is the thickness of the layer between these two zones. Much of what mid-level managers previously did to bridge purpose and protocol—writing the memo, shaping the narrative, drafting the plan, synthesizing inputs—is now potentially encodable. That does not mean those managers disappear, but it does mean their comparative advantage shifts away from production of text and toward curation, calibration, and contestation of machine-generated text.</p><p>Empirically, one sees this boundary shift in the way organizations talk about “AI copilots.” They are rarely framed as replacing decision-makers; instead, they are positioned as ever-present assistants: always there to draft, summarize, suggest, and remind. The managerial problem becomes not “how to execute all this work” but “how to ensure that what is cheaply executable is still aligned with what we actually intend.”</p><p>In that sense, LLMs complete the arc begun by deterministic software. Where earlier systems subsumed the procedural skeleton of management into code, LLMs begin to subsume its linguistic flesh. The organization’s language becomes, in large part, a generated artifact. What remains distinctly managerial is the ongoing decision about which generated worlds to inhabit.</p><h2><strong>3. The Emergence of Second-Order Management</strong></h2><p>Once large language models become part of the organizational stack, management no longer confronts software primarily as a deterministic executor of pre-specified rules. It confronts software as a probabilistic interpreter: a system that generates meanings, classifications, and rationales under uncertainty. If deterministic systems subsumed procedural discretion (the “how” of execution), LLMs reach into interpretive discretion (the “what it means” that used to precede execution). But because these models hallucinate, misclassify, and misalign in ways that cannot be fully predicted ex ante, they cannot simply be treated as new protocols. They must be treated as <em>objects of management</em> in their own right.</p><p>Previous “first-order management”—specifying rules, drafting policies, writing narratives—becomes heavily model-mediated. Second-order management—designing, overseeing, and constraining how models participate in that work—becomes the distinctive managerial function. The manager’s problem is no longer only “how do we translate intent into action?”; it is also<strong> “how do we govern the probabilistic systems that now perform much of that translation for us?”</strong></p><p>This section argues that second-order management is a structurally necessary response to probabilistic protocolization. It emerges from four linked dynamics: the need for epistemic oversight of model behavior; the requirement to build protocols for using protocols; the rise of explicit model-governance roles; and a redefinition of management as the mediator between human intent and machine interpretation.</p><h4><strong>3.1 Oversight of Probabilistic Systems: Epistemic Vigilance as Core Task</strong></h4><p>Deterministic systems fail predictably: if the rule is wrong, the outcome is wrong in the same way, every time. LLMs fail stochastically. They hallucinate citations, misread edge cases, and produce fluent but subtly biased or incomplete summaries. The same prompt, on a different day or model version, may produce a different answer. The empirical “jagged frontier” results—high performance inside certain task regions, confident nonsense just beyond them—are not pathologies around the edges; they are a structural feature of probabilistic modeling.</p><p>In such an environment, any organization that uses LLMs for interpretive tasks inherits a new risk surface:</p><ul><li><p>model drift over time as vendors update weights and training sets</p></li><li><p>context drift as organizational norms evolve while fine-tuned models lag</p></li><li><p>usage drift as employees discover new prompts, workarounds, and shadow workflows</p></li></ul><p>These drifts cannot be fully neutralized by better training alone because they partly arise from how humans <em>co-evolve</em> with the tool. Staff will adapt their behavior to what “seems to work” with the model; prompts will converge toward shortcuts that produce desirable-looking outputs, not necessarily truthful or well-calibrated ones.</p><p>Under these conditions, oversight is not a matter of sporadically “checking for bugs.” It is a continuous <em>epistemic practice</em>: monitoring where model behavior is diverging from organizational intent, where confidence is decoupled from correctness, and where generated narratives are silently reshaping how the organization understands itself.</p><p>Crucially, this cannot be outsourced entirely to automated evaluators, because evaluators share the same ontological limitations as the models they assess. Benchmarks are narrow; real work is messy. Nor can it be ceded to vendors, whose incentives and visibility into local context differ from those of the organization. The structurally appropriate locus for this vigilance is management, because:</p><ul><li><p>Managers are accountable for outcomes shaped by model-mediated interpretations (e.g., customer harm, regulatory violations, reputational damage).</p></li><li><p>Managers sit at the intersection of domains: they see both model behavior and its organizational consequences.</p></li><li><p>Managers already own the “is this good enough for our purpose?” threshold; probabilistic systems simply widen the class of outputs that must pass through that gate.</p></li></ul><p>Thus, oversight of probabilistic systems is not a new specialty bolted onto management; it is a re-specification of an old task (judging the adequacy of interpretations) under new conditions (interpretations are now mass-produced by models).</p><p>Practically, this oversight includes:</p><ul><li><p><strong>Monitoring drift</strong> in model behavior through domain-specific evaluation sets, not just generic benchmarks.</p></li><li><p><strong>Validating outputs</strong> in high-stakes contexts (law, compliance, safety, HR) with explicit human-in-the-loop review.</p></li><li><p><strong>Designing fallback mechanisms</strong> when model confidence is low or inputs deviate from the training manifold (e.g. “abstain and escalate” behaviors).</p></li><li><p><strong>Adjudicating conflicts</strong> when model inferences contradict encoded policy or human judgment.</p></li></ul><p>The point is not that managers read every output. It is that they design and maintain the <em>conditions under which model outputs are trusted, checked, or rejected</em>. That is a second-order function.</p><h4><strong>3.2 Protocols for Using Protocols: The Meta-Design Problem</strong></h4><p>LLMs intensify a paradox that was already latent in the earlier subsumption thesis. If organizations are increasingly governed by protocols, then the design of those protocols is itself a domain of governance. With LLMs, the paradox becomes explicit: we need <strong>protocols for using probabilistic protocols</strong>.</p><p>Concretely, organizations must answer questions such as:</p><ul><li><p>For which decision classes may LLM outputs be used as <em>inputs</em> (brainstorming, first drafts, candidate options)?</p></li><li><p>For which decision classes may LLM outputs be used as <em>decisions</em> (auto-tagging, low-risk routing)?</p></li><li><p>Under what conditions is human verification <em>mandatory</em> (regulatory exposure, employment decisions, safety-critical recommendations)?</p></li><li><p>Which tasks are <em>non-delegable</em> to LLMs, not for technical reasons but because delegation itself would undermine legitimacy (e.g. disciplinary actions, significant layoffs, ethical judgments)?</p></li><li><p>How should exceptions created by model error be escalated, logged, and fed back into model and policy revision?</p></li></ul><p>These are meta-protocol design questions: they specify the rules that govern when and how rules generated or mediated by models may be used.</p><p>This raises the specter of infinite regress: if protocol design is itself protocolizable, why not ask an LLM to propose the policies that govern LLM usage, and so on ad infinitum? In practice, organizations already do this—“draft an AI use policy” is a common internal prompt. But the regress stops where <em>accountability</em> and <em>legitimacy</em> are at stake. At some point, someone must sign their name under “we will use LLMs for X, but not for Y.” That act of acceptance cannot be delegated to the system without collapsing the distinction between the governed and the governor.</p><p>In other words, meta-protocols can be <em>drafted</em> by models, but they cannot be <em>owned</em> by them. Ownership of meta-protocols belongs to management because:</p><ul><li><p>They encode trade-offs among speed, cost, risk, and values.</p></li><li><p>They delimit which human roles remain responsible for which harms.</p></li><li><p>They define the organization’s posture toward regulators, customers, and employees when model-mediated failures occur.</p></li></ul><p>Designing protocols for using probabilistic protocols is therefore the core of second-order management. It is where management reasserts itself as the locus of “ought” in a landscape where an increasing amount of “is” arrives pre-formatted by models.</p><h4><strong>3.3 The Rise and Limits of Model Governance Roles</strong></h4><p>As LLMs pervade organizational workflows, explicit model-governance roles emerge: AI risk leads, domain stewards, prompt and retrieval owners, red-team leads, “AI councils,” and cross-functional committees. Superficially, these look like a rebranding of existing risk and compliance functions. The question, for a hard thesis, is whether they are structurally new or merely incremental.</p><p>There is continuity. Model governance extends familiar tasks:</p><ul><li><p>translating regulation into internal controls</p></li><li><p>performing audits and red-team exercises</p></li><li><p>maintaining documentation and incident logs</p></li></ul><p>But there are also genuine discontinuities:</p><ol><li><p><strong>The object of governance is interpretive, not just procedural.</strong> Traditional internal control focuses on whether processes were followed. Model governance must also ask whether <em>the interpretations produced by the model</em> are systematically biased or misaligned. That is closer to epistemology than to classical process audit.</p></li><li><p><strong>The system’s behavior evolves as a function of use.</strong> Because LLM behavior is co-shaped by prompts, retrieval context, and user adaptation, “the model” is partly a moving target. Governance must therefore cover <em>socio-technical ensembles</em> (model + prompts + data + user practices), not just a static artifact.</p></li><li><p><strong>The tools can write their own constraints.</strong> LLMs can propose their own usage policies, ethical guidelines, and prompt libraries. Model governors must decide which of these self-descriptions to accept—another layer of reflexivity absent in traditional systems.</p></li></ol><p>The model-governor is responsible for aligning model-mediated behavior with organizational purpose along several dimensions:</p><ul><li><p><strong>Prompt and instruction governance.</strong> Deciding which system messages and instruction templates define the “house voice” and risk posture (e.g. always cautious in legal matters, always conservative on safety).</p></li><li><p><strong>Domain adaptation.</strong> Curating which internal documents, cases, and examples are allowed to shape model behavior in specific domains, recognizing that this choice implicitly encodes a theory of the business.</p></li><li><p><strong>Red-teaming and adversarial testing.</strong> Actively probing for failure modes—bias, manipulation, leakage, unsafe recommendations—and feeding these back into technical and policy changes.</p></li><li><p><strong>Interpretability and explanation assessment.</strong> Not in the strict ML sense of full mechanistic transparency, but in the organizational sense: can we explain to regulators, courts, or affected parties why the system behaved as it did? If not, are we comfortable deploying it in this domain at all?</p></li><li><p><strong>Values alignment.</strong> Making explicit which organizational values constrain model usage. For example: “We will not allow the model to optimize engagement at the expense of mental health; we will not deploy automated persuasive messaging in contexts involving vulnerable groups.”</p></li></ul><p>These tasks are definitionally non-automatable because they concern <em>which world the organization chooses to inhabit</em>. The model can propose many internally coherent norm packages; only management can say “we choose this one, and we accept the opportunity costs.”</p><p>Thus, model governance roles are not a side-branch of IT; they are a crystallization of second-order management. Where 20th-century management designed organizational structure, 21st-century management increasingly designs <em>model structure plus organizational structure</em> as a coupled system.</p><h4><strong>3.4 Management as Mediator Between Human Intent and Machine Interpretation</strong></h4><p><strong>In the deterministic software era, managers mediated between human intent and machine execution</strong>: “here is what we want to happen; here is how we configure the system to do it.” LLMs shift the locus of mediation. They generate candidate <em>interpretations</em> of situations, policies, histories, and futures in the same language managers use to think and justify. <strong>As a result, the future manager’s job becomes to mediate between human intent and </strong><em><strong>machine interpretation</strong></em><strong>.</strong></p><p>This difference is not cosmetic. It changes what “discretion” means.</p><ul><li><p>With deterministic code, discretion appears mainly at the edges: configuring parameters, handling exceptions when the process cannot run.</p></li><li><p>With LLMs, discretion appears <em>within</em> interpretation: deciding which model-generated framing of a situation the organization will treat as real.</p></li></ul><p>Consider three sites where this mediation is visible:</p><ol><li><p><strong>Sense-making.</strong> Models can generate multiple plausible stories about the same metric shift (“churn rose because of X, Y, or Z”). Choosing which story to act on is not a purely analytical decision; it is a bet about the world and about the organization’s identity. Second-order management is what refuses to let “whichever story the model says first” become the de facto truth.</p></li><li><p><strong>Norm articulation.</strong> Models can write values statements, codes of conduct, and ethical guidelines that sound like those of comparable organizations. Managers must decide whether to accept this <em>normative autopilot</em> or to assert a distinctive stance, even at the cost of more work. A generated mission can be inspiring but empty; second-order management is the practice of saying “no, this is not us,” even when the text is perfectly polished.</p></li><li><p><strong>Legitimacy and responsibility.</strong> When model-mediated decisions cause harm, someone must stand in front of employees, customers, or regulators and explain what happened. Saying “the model did it” is organizationally and legally inadequate. The human manager remains the bearer of justification, even when the concrete interpretive labor was done elsewhere.</p></li></ol><p>Second-order management is therefore not residual or merely supervisory. It is a <em>new generative layer</em> created by the attempt to finalize the subsumption of managerial discretion. The more organizations ask LLMs to “do the interpretation for us,” the more they create a need for actors who:</p><ul><li><p>decide which interpretations are admissible</p></li><li><p>determine where model silence is preferable to model speech</p></li><li><p>maintain the gap between what the system outputs and what the organization is willing to endorse</p></li></ul><p>In that sense, LLMs both complete and break the earlier genealogy. They complete it by extending subsumption into interpretive domains. They break it by revealing that some part of management—purpose-setting, value judgment, legitimacy-production—cannot be encoded without rendering the very idea of responsibility incoherent. That remainder is where second-order management lives: in the ongoing, uncomfortable work of saying “this is what the model says, but this is what we mean.”</p><h2><strong>4. Reopening the Problem of Intent</strong></h2><p>The earlier genealogy treated “intent” as the upstream anchor of organization: in Ibn Khaldun’s king-system, asabiyyah gives rise to sovereign purposes; in bureaucratic modernity, intent is textualized as policy; in the corporate era, intent condenses into Drucker’s “theory of the business” and Porter’s strategic positioning; in the software epoch, intent is increasingly encoded in architectures and protocols that execute themselves. Throughout, the ontology is stable: there is something like a system-level “want,” and management is the interpretive layer that connects that “want” to the downstream field of action.</p><p>Large language models disturb this picture. They do not simply execute intent; they infer it, simulate it, and sometimes overwrite it, all in the same medium in which intent is articulated: natural language. They make organizational purposes appear as things that can be prompted, sampled, rephrased, and optimized, rather than as slow, contested, collectively constructed commitments. In doing so, they reopen the problem of what “intent” even is in an organization whose internal discourse is heavily machine-shaped.</p><p>This section argues that LLMs destabilize intent along four axes. First, they multiply and personalize “intent” by generating tailored micro-purposes at the level of individual prompts and use cases. Second, they unbundle declared intent from encoded, emergent, and inferred intent, forcing organizations to manage an “intent stack” rather than a single coherent mission. Third, they erode the temporal stability of intent by enabling rapid re-articulation and A/B testing of purpose statements, strategy narratives, and value framings. Fourth, they complicate legitimacy by making it harder to tell whether an organization’s stated intent is a normative commitment or a convenient sample from a distribution of plausible-sounding scripts.</p><p>The result is not “no intent,” but intent as a contested, probabilistic, multi-layered construct. Management, already recomposed as second-order oversight of models, now becomes the site where this destabilized ontology must be held together long enough for collective action to remain possible.</p><h4><strong>4.1 From Singular Mission to Plural, Sampled Intent</strong></h4><p>Classical management theory presumes that, at any given moment, an organization has—or ought to have—a relatively coherent purpose. Drucker’s “theory of the business” is wrong or right, but it is singular; Porter’s strategic intent is contested, but the point of strategy is to converge on a position; even Lean Startup’s iterative model presupposes that each experiment is run against a provisional, explicit hypothesis about what the firm is trying to learn.</p><p>LLMs cut across this singularity in two ways.</p><p>First, they personalize discourse. In the enterprise context, LLM-based assistants already generate different articulations of “what we are doing” for different audiences: one version of the product vision for engineers, another for marketing, another for regulators, another for customers, each tuned to their language and concerns. Tools like Microsoft’s Copilot, Google’s Gemini for Workspace, and Salesforce’s Einstein layer generative models on top of existing communication channels precisely to tailor messages to audience and context at scale. The same underlying “intent” is thus instantiated as thousands of variants, generated on demand, each locally coherent and rhetorically optimized.</p><p>Second, they make “intent” appear on demand anywhere language is needed. A product manager asks, “Draft a problem statement and success metrics for this initiative.” A policy lead prompts, “Write a purpose clause for our AI governance charter in the style of comparable firms.” A founder asks, “Give me three candidate mission statements emphasizing sustainability, growth, or innovation.” Each prompt elicits a plausible articulation of intent. None is necessarily false; each expresses a different slice of the space of “things an organization like this might want.”</p><p>Operationally, then, intent becomes something that is sampled whenever needed, not a fixed, prior object that all language must faithfully represent. The model does not ask, “What do we, as this particular organization, deeply commit to?” It asks, “Given this prompt and context, what is a likely next sentence?” That logic creeps back into the organization’s self-understanding. The mission becomes a menu.</p><p>This is not wholly new—consultants have been generating bespoke mission statements for decades—but LLMs change the cost structure. Generating ten different plausible intent narratives is now cheaper than deciding which one to own. The bottleneck shifts from articulation to commitment.</p><p>From a subsumption perspective, this is an inversion. Previously, managerial discretion was absorbed into protocols that enacted a single, declared intent more consistently. Now, model-mediated language generation makes it easy to produce an abundance of quasi-intents—locally persuasive framings that may or may not share a coherent core.</p><p>The ontology of intent becomes plural and contextual: there is no longer one “intent”; there are many candidate intents, continuously instantiated and re-instantiated in language, some human-authored, some machine-generated, all competing for adoption.</p><h4><strong>4.2 Intent: Declared, Encoded, Emergent, Inferred</strong></h4><p>To make sense of this pluralization, it is useful to decompose “intent” into layers. In a post-LLM organization, at least four distinct layers can be analytically separated, even if they blur in practice:</p><ol><li><p><strong>Declared intent</strong>: what leaders say the organization is about—visions, strategies, policies, values statements, public commitments.</p></li><li><p><strong>Encoded intent</strong>: what protocols and architectures actually enforce—approval rules, incentive schemes, KPIs, access controls, smart contracts.</p></li><li><p><strong>Emergent intent</strong>: what the organization effectively pursues in practice, as revealed by patterns of behavior over time—where resources actually flow, which customers are favored, which risks are tolerated.</p></li><li><p><strong>Inferred intent</strong>: what models believe the organization wants, inferred from prompts, documents, and interaction data—and thus what they optimize for in their outputs.</p></li></ol><p>Before LLMs, management already struggled with gaps between declared, encoded, and emergent intent. “We care about safety” collides with incentives that reward speed; “we are customer-centric” coexists with architectures optimized for internal convenience. Organizational sociology is, in large part, the study of these gaps.</p><p>LLMs add a new layer: inferred intent. A domain-adapted model trained on past incident reports may infer that the organization’s “real” priority is uptime over user privacy because most historical trade-offs went that way; a sales copilot may infer that closing deals at quarter-end is more important than long-term margin integrity because past emails show that behavior is praised. These inferences are not explicit; they are encoded in the distribution of outputs.</p><p>Practically, this means that even if leadership revises declared intent—new vision, new values—LLM-mediated systems may continue to produce interpretations and recommendations that reflect earlier emergent intent. The model’s internal representation of “what we care about” lags behind, unless explicitly retrained or constrained.</p><p>The “intent stack” becomes a governance object in its own right:</p><ul><li><p>Declared intent can be updated with a memo.</p></li><li><p>Encoded intent can be updated by changing rules, code, or architectures.</p></li><li><p>Emergent intent shifts via changes in allocation and practice.</p></li><li><p>Inferred intent shifts via changes in training data, prompts, and feedback loops.</p></li></ul><p>The destabilization arises because these layers can move out of sync in new ways. An org can have:</p><ul><li><p>A progressive declared intent (e.g. strong stance on fairness).</p></li><li><p>Legacy encoded intent (e.g. incentive schemes that reward volume).</p></li><li><p>Emergent intent that quietly follows the incentives.</p></li><li><p>LLMs that, trained on the last five years of docs and tickets, infer that “success” means more volume, and generate narratives that rationalize that behavior.</p></li></ul><p>From the outside, the organization sounds aligned; internally, its models and protocols are optimizing for a different objective. The epistemic surface is smooth; the ontology underneath fractures.</p><p>Second-order management, in this context, is not just “oversight of models.” It is active synchronization of the intent stack: ensuring that declared, encoded, emergent, and inferred intent do not drift so far apart that collective action becomes incoherent or legitimacy collapses.</p><h4><strong>4.3 Temporal Instability: Intent Under Version Control</strong></h4><p>Intent used to be slow. Mission statements were revised on the scale of years; strategic plans were refreshed annually; value statements lingered on office walls long after they ceased to inspire. This slowness had costs—rigidity, denial—but it also provided a temporal backbone. People could organize their expectations around the idea that “what we are about” would not change every quarter.</p><p>LLMs introduce a peculiar kind of temporal fluidity.</p><p>On the one hand, they make it easy to re-articulate and “update” intent continuously. Leadership can A/B test value framings in internal town halls, adjust messaging in response to engagement metrics, and iterate on strategic narratives with the same flexibility that growth teams A/B test landing pages. The temptation to treat mission as a UX parameter is real: “Which version of our intent narrative yields higher employee alignment scores?” becomes a plausible analytic question.</p><p>On the other hand, models themselves version over time. Vendors roll out new base models; organizations fine-tune on new data; guardrails and prompts change. As a result, the model’s outputs—its effective articulation of what counts as relevant, valuable, aligned—shift even if the official mission has not. A leadership team may think purpose is stable, but the generative layer that sits between that purpose and everyday language has been silently updated twice this quarter.</p><p>The net effect is temporal instability in two directions:</p><ul><li><p><strong>Top-down volatility</strong>: leadership can (and may be incentivized to) tweak intent framings frequently, because the cost of doing so is low. There is always a new slide, a new tagline, a new “north star” narrative to test.</p></li><li><p><strong>Bottom-up drift</strong>: as LLM-mediated workflows accumulate, the practical meaning of intent in everyday decisions drifts with model updates and accumulated prompts.</p></li></ul><p>You get the worst of both worlds: an intent that is rhetorically agile but ontologically slippery.</p><p>This is not an argument for freezing language. It is, however, a recognition that the very cheapness of re-articulation introduces a new managerial responsibility: deciding when not to change the story, even when the tools make change easy; deciding when to lock in a statement of purpose and give it time to bite into practice.</p><p>From a governance standpoint, this suggests that “intent versioning” might need to become explicit. Just as organizations maintain version histories for code and policy, they may need to maintain versioned, auditable histories of declared intent and the model configurations that operationalized it at any given time. Without that, post-hoc accountability—“what did we think we were doing when we made this model-mediated decision?”—becomes impossible.</p><h4><strong>4.4 Alignment as Intent Negotiation, Not Intent Imposition</strong></h4><p>Alignment, in the machine-learning sense, is usually framed as “making the model do what we want”: using RLHF, constitutional AI, or similar techniques to nudge behavior toward human preferences. That framing presupposes that “what we want” is clear and stable enough to be a training signal.</p><p>In organizations, this is rarely the case. Intent is contested internally—across functions, geographies, ranks—and externally, among regulators, customers, investors, and other stakeholders. Even within a single domain like “fairness,” multiple incompatible principles compete (equality of opportunity, equality of outcome, group parity, individual desert). “What we want” is not a scalar; it is a field of tension.</p><p>LLMs render this tension unavoidable because they immediately surface the multiplicity of plausible “wants.” Ask a model, “What should our AI ethics policy emphasize?” and it will happily list safety, fairness, privacy, transparency, human oversight, autonomy, etc. Ask it to write three alternative policies emphasizing innovation, risk minimization, or user empowerment, and it will oblige. The organization is confronted with its own undecidability.</p><p>Under these conditions, “model alignment” is less a matter of encoding a single, pre-existing intent and more a matter of mediating among multiple, partially incompatible intents. The manager’s role is not to transmit a fixed “ought” into the loss function but to adjudicate which “ought”s win when they conflict, and which sacrifices are acceptable.</p><p>Among other things, this means:</p><ul><li><p>Alignment work becomes political. Choices about prompts, guardrails, and training data embody substantive value trade-offs, not merely technical tweaks.</p></li><li><p>Alignment work becomes iterative. As emergent behavior and external feedback reveal new tensions, organizations must revisit not only the model but the underlying intent.</p></li><li><p>Alignment work becomes dialogic. Employees, users, and regulators will increasingly demand input into “what the model is allowed to do,” forcing management to treat intent as a site of negotiation, not a decree.</p></li></ul><p>In other words, LLMs transform alignment from an engineering problem into an explicit governance problem. They push the question “what do we actually intend?” to the surface, because every misaligned output forces a conversation about whether the problem lies in the model or in the incoherence of the underlying intent.</p><p>Second-order management lives in this space. It is the practice of treating alignment not as imposition (“make the model obey the mission”) but as ongoing negotiation among declared, encoded, emergent, and inferred intents, under constraints of risk, law, and legitimacy.</p><h4><strong>4.5 Legitimacy and the Performative Fragility of Intent</strong></h4><p>Intent is not just an internal planning object; it is a public claim. When organizations say “we intend to reduce our carbon footprint,” or “our purpose is to empower small businesses,” they are not merely instructing internal processes. They are making promises that others will hold them to.</p><p>LLMs complicate this performative dimension in two ways.</p><p>First, they make it possible to generate highly polished, audience-tailored intent performances at scale. Investor letters, ESG reports, DEI statements, and AI ethics principles can all be drafted, refined, and localized with minimal marginal cost. This raises the baseline of rhetorical sophistication while lowering the cost of insincerity. The gap between “we can say it” and “we will do it” widens.</p><p>Second, they blur authorship. When a mission statement is known to be heavily machine-assisted, its status as a speech act changes. Stakeholders may ask: whose intent does this voice represent—leadership’s, the comms team’s, or the model’s? If a generated diversity statement echoes standard industry boilerplate, is the organization committing to anything beyond what the distribution already encodes?</p><p>Legitimacy, in this environment, requires more than alignment between words and deeds; it requires credible ownership of words whose form could have been machine-generated. That ownership has to be demonstrated by behavior over time, but also by visible acts of authorship: leaders who visibly wrestle with intent, revise it in response to criticism, and tie it to concrete choices.</p><p>Paradoxically, the more machine-shaped organizational language becomes, the more valuable those rare instances of obviously human, idiosyncratic articulation become. A CEO who uses the same stock phrases as everyone else will be assumed to be reading from the same prompt library. A leader who stakes out a specific, non-obvious intent—and then accepts costly constraints because of it—signals that something like real purpose is present.</p><p>Second-order management, in this sense, includes the curation of where and when to <em>refuse</em> machine mediation. Not all intent-bearing statements should be generated or smoothed by a model. Some must remain rough, specific, and clearly authored if they are to carry legitimacy.</p><h4><strong>4.6 Intent as a Managed Scarcity</strong></h4><p>The subsumption genealogy began with an abundance of discretion and a scarcity of protocol. It then traced how organizations gradually encoded more of their behavior into rules and architectures, reducing the discretionary bandwidth needed to sustain complex coordination. LLMs push this further: they promise an abundance of interpretations, narratives, and candidate intents at negligible cost.</p><p>The scarce resource is no longer articulation; it is committed intent.</p><p>Committed intent, in this sense, has three properties:</p><ul><li><p>It constrains: it rules out otherwise attractive options because “that is not what we do.”</p></li><li><p>It endures: it is stable enough over time to structure expectations and enable learning.</p></li><li><p>It is owned: someone can be held responsible for having chosen it.</p></li></ul><p>LLMs are bad at all three. They are designed to maximize plausibility, not constraint; to be updateable, not enduring; to diffuse authorship, not concentrate it. That is precisely why the project of “automating intent” fails at the point where intent matters most.</p><p>From here, the hard thesis is simple: LLMs destabilize intent by flooding the organization with cheap, plausible substitutes for it, but they cannot replace the scarce act of commitment that turns a sampled sentence into a lived purpose. They make it easier to defer that act—to “let the model say something for now”—but they do not remove its necessity.</p><p>Management, after LLMs, is therefore not merely “interpretation after protocol” but “commitment after simulation.” It is the practice of:</p><ul><li><p>Choosing, among many machine-generated framings, which ones the organization will actually stand behind.</p></li><li><p>Insisting on temporal stability in some parts of the intent stack, even as other layers flex and update.</p></li><li><p>Synchronizing declared, encoded, emergent, and inferred intent enough to keep the organization from dissolving into a collage of prompts.</p></li><li><p>Accepting responsibility for the ways in which model-mediated interpretations shape real-world outcomes.</p></li></ul><p>In that sense, LLMs do not close the genealogy of subsumption; they expose its limit condition. You can encode more and more of management into software, including large tracts of interpretation, but at some point you hit the moment where someone has to say, “This is what we mean,” knowing that no model can guarantee they are right.</p><p><strong>That moment—the reassertion of intent as a human, political, and moral act in a world of probabilistic governance—is where the ontology stabilizes again, if only temporarily. And that is where management, stripped of much of its traditional repertoire, still earns its name.</strong></p><h2>Conclusion</h2><p>Large language models’ management significance lies in how they alter the epistemic conditions under which organizations articulate and act upon their purposes. By generating fluent interpretations at scale, they intrude into the linguistic and narrative space where managerial meaning-making has historically occurred. They do not create organizational intent, but they reshape how intent is expressed, inferred, and contested.</p><p>As a result, much of the textual work of management—drafting, summarizing, framing—will be absorbed into model-mediated workflows. But the core managerial function resurfaces at a higher level: as the curation and commitment required to stabilize meaning in an environment where meaning can always be regenerated.</p><p>This conclusion must be tempered by realism. Organizations have never possessed perfectly coherent intent; politics, incentives, and culture have always produced drift. LLMs do not invent this condition, but they amplify it by lowering the cost of alternative, plausible framings. Nor will LLMs fully penetrate interpretive labor: much of what managers do remains relational, embodied, or tacit, beyond the reach of probabilistic text models.</p><p>Yet even with these limits, LLMs force a reframing of management theory. They reveal that the boundary between human and machine interpretation is porous, and that managerial authority increasingly consists not in producing interpretations but in choosing among them, owning them, and bearing the consequences they entail. Commitment—rather than articulation—becomes the scarce resource.</p><p>In this sense, the LLM era does not close the genealogy of managerial subsumption; it exposes its limit. <strong>Software can execute protocols; models can simulate narratives; but only managers can decide which of these narratives will guide collective action.</strong> In an organizational landscape saturated with machine-generated meaning, management becomes the practice of saying, with clarity and responsibility: <em>this is what we mean.</em></p>