<p><em>The executives who lead Assurance’s major business units own their performance outcomes. Now the CIO wants to standardize and orchestrate the firm’s rapidly proliferating, and consistently diverging, AI agents. The enterprise may gain speed, consistency, and regulatory clarity—but business leaders may lose control over how decisions are made. How should this conflict be managed?</em></p><h2>An Incident at Assurance</h2><div><hr></div><p>When Carol Callow, Group CIO of Assurance, stepped into the firm’s primary operations center in London, she stopped mid-stride. The floor was vast, brightly lit, and unusually quiet for midmorning. Long rows of desks stretched toward the windows overlooking the Thames, each occupied by analysts focused on wide, high-resolution screens. Claims queues updated in real time. Underwriting dashboards pulsed softly with new data. Along the margins of several displays, narrow side panels refreshed continuously with short blocks of text—summaries, flags, recommendations.</p><p>“There must be fifty of them running in here,” said Priya Bommar, Assurance’s head of Digital Operations, gesturing casually toward the screens. “Fifty what?” Callow asked, though she already suspected the answer. “Agents,” she said. “All internal. Different workflows, different models. They’re just assisting—triage, summarization, prioritization.” Callow leaned closer to one screen. A complex commercial claim flickered briefly, then shifted into a different queue. A small annotation appeared beneath it: <em>Escalated—Inconsistency Detected</em>.</p><p>“Who made that call?” she asked. “The system flagged it,” Priya replied. “The handler accepted the recommendation.” Callow frowned. Assurance had approved the use of internal AI agents this year, but only under strict conditions. No autonomous decisions and no customer-facing actions. Agents were embedded inside existing applications and case manager tooling. They authenticated through existing service accounts. From an audit perspective, nothing had changed. Every action still resolved to an approved system identity.</p><p>Yet watching the claim move across the screen, Callow felt the shift more acutely than any board presentation had conveyed.</p><p>As they walked the floor, Priya pointed out performance improvements. Average resolution times were down nearly 20%. Backlogs that once lingered for weeks were now cleared in days or hours. Customer satisfaction reflected this improvement; NPS scores had climbed steadily.</p><p>“In practice,” she said, “people trust the agents. They’re consistent, conservative, and even creative. They know the protocols better than most of our senior case managers”</p><p>Callow stopped again, this time in front of a cluster of underwriting stations. Several recommendations were marked <em>Auto-Generated</em>. </p><p>“Who owns the logic behind that?” Callow asked. Priya hesitated. “The application team. Same as any other component.” She smiled sincerely.  “And who owns the judgment?” Priya responded, “The business unit.”</p><p>The answer felt insufficient, though Callow could not yet articulate why.</p><p>As of last year, Assurance’s largest European competitor had begun using internal AI agents aggressively across claims, underwriting, and customer service. Analysts credited the shift with faster cycle times and improved loss ratios. Assurance’s board, historically cautious about new technology, began to worry that their technologically conservative culture was becoming a competitive liability.</p><p>Rather than impose a centralized AI program, Assurance chose a federated approach, in alignment with their enterprise organizational design. Each major business unit—commercial insurance, personal lines, specialty risk—could deploy agents within a shared technical framework. Their shared services IT department provided infrastructure and access to approved models. Legal reviewed system prompts. Compliance signed off on controls and did regular audits. Meanwhile Assurance’s identity, permissions, and audit trails remained unchanged.</p><p>The rollout was deliberately quiet. No press releases. No internal rebranding. The agents appeared as feature enhancements. For months, they behaved like highly capable assistants—drafting summaries, highlighting anomalies, ranking priorities.</p><p>Over time, however, differences emerged. In Northern Europe, underwriting recommendations skewed conservative. Approval rates dipped slightly, but loss exposure fell. In Southern markets, approval rates rose sharply and growth accelerated. Audits confirmed both were compliant; both relied on approved agent software and were within tolerance bounds of case management guides.</p><p>“All actions are authorized,” said Martin Keller, head of Compliance. “Every escalation traces back to a valid service account.”</p><p>“But the decisions are materially different,” said Elaine Porter, Assurance’s Chief Risk Officer. “And no one seems to own that difference.”</p><p>“That’s business judgment,” Martin replied. “The agents surface information. People decide.”</p><p>The issue remained unresolved. Then, a highly visible incident raised further concerns: A multinational client filed a complex commercial claim involving overlapping jurisdictions and historical policy amendments. The claim was initially routed through standard channels, then flagged by an agent supporting the regional claims team. The agent recommended escalation and extended review, citing similarities to prior fraud cases. The handler accepted the recommendation without hesitation. The claim stalled and the client complained. Within weeks, a regulator requested an explanation.</p><p>During the internal review, the case analysis proved defensible. The escalation logic was statistically sound. Yet no one could clearly explain who had decided to act. The logs showed the escalation as originating from <em>ClaimsApp-EU</em>, an application identity owned by a team that had not reviewed the agent’s behavior in months.</p><p>Legal discovered that no policy explicitly defined the authority of agent recommendations. HR, when consulted, expressed confusion.</p><p>“This isn’t a personnel issue,” they said. “There’s no role involved.”</p><p>Yet the client wanted accountability. Regulators requested clarity. And inside Assurance, business unit leaders began to push back.</p><p>On the drive back from a tense internal meeting, Callow recalled a conversation with Sarah Whitcombe, head of Commercial Lines.</p><p>“My team understands this market,” she had said sharply. “The agent reflects our data, our history. If you standardize this, you’re taking away my ability to manage risk.”</p><p>Callow understood. Each unit had tuned its agents differently—prompt structures, retrieval sources, escalation heuristics. These were not arbitrary choices. They reflected years of accumulated expertise and local judgment.</p><p>But the board saw something else. “If something goes wrong,” the CFO asked bluntly, “who do we point to?” No one answered.</p><p>As the taxi pulled into the driveway of his hotel after a morning of back-to-back meetings, Callow rubbed her eyes. “I’m exhausted,” she muttered, leaning back against the leather seat. “I wonder what time it is in New York.” “It’s 6:47 a.m.,” his driver replied.</p><p>Callow stared out the window. The agents were delivering value. The controls were intact. Yet the sense of drift was unmistakable.</p><div><hr></div><h3><strong>Change or Drift</strong></h3><p>“Damn,” Callow muttered the next morning. She rinsed her face and stared at his reflection in the mirror of the airline lavatory.</p><p>Since boarding the plane, she had been wrestling with an uncomfortable realization: imposing centralized governance on Assurance agents would not be straightforward. The firm was simply too decentralized.</p><p>Founded decades earlier, Assurance had grown by acquisition. Each business unit operated almost like a subsidiary, with its own leadership, processes, and case management nuances. Unit heads owned their outcomes—and their tools.</p><p>Sarah was typical. Her knowledge of commercial risk markets commanded deep respect. Over the years, she had worked closely with her teams to tune agent behavior—adjusting prompts, refining escalation thresholds, selecting data sources. Her people trusted the systems because they felt familiar.</p><p>When Callow had raised concerns after the incident, Sarah had bristled. “Why are we still using this escalation logic?” she had asked.</p><p>“Because it works,” she replied. “We’ve refined it over time. Until this incident, it’s been reliable.”</p><p>Having once run a business unit himself, Callow understood the bind. Strip leaders of control while holding them accountable, and resistance was inevitable.</p><p>“I can’t just centrally control every tool and agent configuration,” she thought. “Sarah might leave.”</p><p>Others would resent it too. They might comply on paper while quietly working around new rules. A heavy-handed initiative could fail spectacularly—and publicly.</p><p>Yet doing nothing was no longer an option.</p><p>“Change or drift,” she thought. Drift would compound risk. “Change it is. But how?”</p><p>Settling back into her seat, Callow opened a magazine to an article describing how a global bank had appointed an executive responsible for enterprise AI systems. The piece portrayed the role as decisive, authoritative—a single point of accountability.</p><p>“That’s the kind of leader I need, but seems like overcorrecting” Callow thought. She knew how Sarah and the other unit heads would react. Setting a single control for AI tooling would feel similar to setting a single leader responsible for excel spreadsheets.</p><p>“I could ask them to volunteer agent workflows for review,” she thought. “If one unit sees benefits, the others might follow.” She put on his headphones and closed her eyes.</p><div><hr></div><h3><strong>The Orchestrator</strong></h3><p>Grigio, Callow’s favorite place for a long business lunch, was flooded with noon sunlight. She ordered the steak. Her guest, <strong>Ravi Chandry</strong>, chose the scampi.</p><p>Ravi had been recommended by a board mentor as someone who had centralized AI governance at a global financial services firm. Recently “retired,” Ravi was rumored to be restless.</p><p>Callow described Assurance’s situation. Ravi listened, then began asking sharp questions—about competitors, regulatory exposure, variance in decision outcomes. Callow did his best to answer.</p><p>Ravi leaned back and said “There’s no question you need to act,” he said. “Your decision-making is fragmented and drifting. You’re carrying risk you don’t see. But you already know that.”</p><p>When Callow raised concerns about unit resistance, Ravi smiled thinly.</p><p>“I understand. You’re threatening their power. But your competitors are moving fast. If you wait, you’ll lose leverage—with regulators and with your own people.” Callow considered her response.</p><p>“I’ve spoken with the board,” he said. “We can make this attractive.”</p><p>Ravi’s expression hardened.</p><p>“Daniel, you need an orchestrator,” he said. “And I’d only do this if the unit heads receive a clear message: this is not optional. You don’t have time for consensus.”</p><div><hr></div><h3><strong>Caution</strong></h3><p>Back in her office, Callow sketched an organizational chart. He wrote Ravi’s name at the top, then paused. Instead, he wrote Michael Trent, head of a smaller but respected unit. Michael was trusted, apolitical, methodical.</p><p>Could he lead this effort? Would he want to?</p><p>When Callow met with Michael, he laid out the numbers. Orchestrating autonomous software could reduce variance and risk significantly. It would help align policy interpretation, but likely at the expense of local agency. Savings could be reinvested in the units. Michael listened, then stood and began diagramming agent workflows on the whiteboard. The board quickly filled with arrows, annotations, dependencies.</p><p>“This is complex,” he said. “Even small changes ripple outward. My advice? Go slow. Start with low-risk workflows. Build credibility.” </p><p>Callow nodded. Michael understood the system. Later, during a review with Sarah and the CFO, tensions flared.</p><p>“Why does it still take weeks to resolve certain claims?” the CFO asked. “Because quality takes time,” Sarah snapped. “And our agents reflect that.” </p><p>“You’re defensive,” the CFO replied. “No,” she said. “You’re stepping into my role.” Callow intervened, but the message was clear.</p><p>As Sarah left, the CFO turned to him. “They all have their pathologies,” he said. “Taken together, they’re putting this company at risk.” Callow sat back, silent.</p><div><hr></div><p><em><strong>When agents drive decisions but don’t “exist” in the org chart, who owns their judgment—and how should Assurance centralize execution guardrails to be accountable without breaking the federated model that makes the business work?</strong></em></p><div><hr></div><h2>Automated Case Answer for Critique</h2><p>At the root, Assurance’s dilemma is not a disagreement about technology. It is an unresolved question about authority. The agents have not introduced a new kind of error or risk; they have surfaced a pre-existing organizational ambiguity that was previously masked by human judgment and slower tempo. </p><p>Decisions are now being shaped by systems that are neither owned nor governed in the same way as people, yet whose effects are indistinguishable from managerial judgment. The conflict between Carol Callow and the business unit heads is therefore not a turf war over tools. It is a structural failure to specify who is allowed to decide, under what authority, and with what accountability when judgment is partially delegated to software.</p><p>The instinct to frame this as a choice between centralization and federation is misleading. Assurance already has centralized elements—identity, compliance, legal review, audit—and highly decentralized ones—risk appetite, market expertise, and outcome ownership. The agents sit uncomfortably within and across these layers. They execute locally, adaptively, and continuously, but inherit identities and permissions that were designed for static applications. As a result, execution has become centralized in effect while authority remains diffuse in theory. This is why the firm experiences “drift”: no one is violating policy, but no one can explain why outcomes differ or who is responsible for aligning them.</p><p>The first step Callow must take is to make explicit a distinction the organization is currently blurring: execution, judgment, and accountability are not the same thing, and they do not need to live in the same place. Execution is already delegated to agents and tools; that genie will not go back in the bottle. Accountability, however, still sits with business unit leaders and, ultimately, the board. The missing layer is judgment: the set of interpretive choices that translate policy and data into action. At present, Assurance is pretending that judgment remains entirely human, even as agents increasingly shape which options are surfaced, which cases are escalated, and which paths are slowed or accelerated. This pretense is what regulators, clients, and executives alike are now finding unsatisfactory.</p><p>Attempting to “centralize the agents” wholesale, as Ravi suggests, would fail for precisely this reason. A single orchestrator with broad control would inherit responsibility for judgments they do not possess the context to make. Business unit leaders like Sarah would experience this as an expropriation of expertise without a corresponding transfer of accountability. Worse, it would create a brittle bottleneck: a central team would be forced to encode local judgment into global rules, freezing what should remain adaptive. The comparison Callow makes to appointing a single owner for spreadsheets is apt. The problem is not the existence of many spreadsheets; it is the lack of shared conventions about when they can be used, how they are trusted, and who stands behind their outputs.</p><p>At the other extreme, relying on voluntary convergence or informal coordination will also fail. The agents are already too consequential, and the external scrutiny too high, for drift to be tolerated. The regulator’s question—“who decided?”—will only become sharper as agents become more capable. An answer that resolves to a dormant service account and a shrugging application team is not defensible. Nor is “the business unit” sufficient when the unit cannot point to a named role, a documented mandate, or a clear chain of authority over the agent’s behavior.</p><p>What Assurance needs, therefore, is not centralized control of agents, but centralized control of <em>how judgment is delegated</em>. This requires introducing a small number of new organizational primitives that sit between tools and people, without anthropomorphizing the agents themselves.</p><p>The most important of these primitives is an explicit notion of agent scope. Not all agents are equal, and treating them as such is a mistake. Some agents merely summarize or retrieve information; others recommend actions that materially affect customers, capital, or regulatory exposure. Assurance should formally classify agents by the type of judgment they influence and the consequences of acting on their recommendations. This classification should be owned centrally and agreed with risk and compliance, but it should not dictate how agents are implemented locally. Instead, it defines the guardrails within which local teams operate.</p><p>Alongside scope, Assurance must introduce the concept of an agent steward. This is not an “AI czar” with unilateral power, nor a shadow manager for every workflow. An agent steward is a named role—likely within each business unit—explicitly accountable for the judgment embodied in a defined set of agents. That steward does not write code or tune models day to day, but they own the interpretive contract between policy and execution. When a regulator asks why a class of claims was escalated differently in two regions, the answer should not be an application name; it should be a person who can explain the rationale, the tradeoffs, and the limits of the agent’s mandate.</p><p>Crucially, these stewards must operate within a centrally defined framework. Callow’s organization should specify what kinds of judgment may be localized and what kinds must be harmonized. For example, escalation thresholds tied to local fraud patterns may reasonably differ; interpretations of global compliance rules may not. This is where centralization belongs: not in the mechanics of agents, but in the contracts that define where divergence is acceptable and where it is not. Central IT and risk functions should enforce these contracts through tooling—common logging, explainability requirements, versioning—but not through micromanagement of prompts or models.</p><p>This approach directly addresses the regulatory problem Assurance faces. Regulators do not require uniform outcomes; they require intelligible ones. By making judgment ownership explicit and scoping divergence deliberately, the firm can explain not just what happened, but why it was allowed to happen that way. Audit trails become narratives rather than forensic puzzles. Compliance moves from checking permissions to validating that delegation matches declared authority.</p><p>Resistance from business unit leaders is inevitable, but it is not insurmountable. Leaders like Sarah are not defending chaos; they are defending judgment. By giving them formal stewardship roles and clear boundaries, Callow can preserve what they value while eliminating silent drift. The key is that stewardship increases responsibility as much as it preserves autonomy. A unit that insists on bespoke agent behavior must also accept explicit accountability for it.</p><p>In practical terms, Callow’s first 90-day move should not be to appoint Ravi or Michael as a universal orchestrator. It should be to convene risk, compliance, and a small number of respected unit leaders to define agent scopes and stewardship roles for the most consequential workflows. Start with claims escalation and underwriting recommendations, precisely because they already attract scrutiny. Require that every such agent has a named steward, a declared scope, and documented rationale for allowed divergence. Instrument these contracts technically, but treat them as governance artifacts, not code.</p><p>Over the longer term, this model has a profound implication for Assurance’s organizational design. Agents will continue to erode the practical distinction between “decision support” and “decision making.” Firms that cling to inherited identity models will find themselves unable to explain their own behavior. Those that succeed will not be the ones with the most centralized AI teams, but the ones that learn to design authority deliberately in a world where judgment is increasingly exercised through machines.</p><p>Assurance’s choice, then, is not between change and drift, but between explicit delegation and accidental delegation. The agents are already acting. The only remaining question is whether the organization will finally decide who stands behind them.</p>