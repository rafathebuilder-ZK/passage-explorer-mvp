<h2>Document Summary</h2><p>It is now an observable fact within NPC Inc. that employees at every level are using personal LLMs and agent tools for work that influences business outcomes. LLM usage is routine and cross-functional. This memo covers the situational context and a policy recommendation for discussion.</p><p>External data consistent with this reality shows that large percentages of employees in other enterprises regularly use personal AI tools for business tasks, often outside corporate visibility, and that this creates observable security and compliance concerns. Independent reporting suggests that as much as three-quarters of employees share proprietary company data into external AI tools when doing work tasks, in many cases without corporate governance or auditing controls.</p><p>Our purpose in this meeting is to establish <strong>a shared factual baseline</strong>, articulate <strong>the real structural tensions and tradeoffs</strong>, and propose <strong>an operating stance</strong> that integrates our product context (NPC Manager and managed agent services), security posture, legal obligations, and organizational design. Managers reading this memo should find that the unwritten assumptions about personal AI use are now explicit, reviewed against real practice, and subject to deliberate governance rather than accident.</p><p>The initial thesis we will examine is this:</p><blockquote><p><em>NPC Inc. should neither ban nor fully internalize employees’ personal use of LLMs. Instead, we should redraw the boundary of accountable cognition—distinguishing private exploratory cognition from organizationally binding commitments—and govern the handoff between the two with explicit protocols.</em></p></blockquote><h2>Situational Context</h2><p>One of the reasons this conversation has risen to the forefront at NPC Inc. is that the corporate suite of AI tools, while significant, does <em>not cover the full spectrum of cognitive work employees undertake</em>.</p><p>Internally provided LLMs, copilots, and situational search tools support defined workflows or structured tasks. They are provisioned, logged, and integrated into our monitoring systems. Yet many employees use external or self-managed tools—personal subscriptions, local models, creative agent configurations not yet sanctioned by the company—because these provide capabilities not yet present in the corporate stack.</p><p>The result is a <strong>shadow AI economy</strong> within the organization, as described by independent security researchers: in practice, most employees already use personal LLM tools for work tasks with more productivity outcomes than official systems.</p><p>NPC Inc.’s current team has good visibility on sanctioned corporate tooling. We see audit logs, integration points, request patterns against internal APIs, and the explicit, formal use of AI tools that touch enterprise data stores. The team does <em>not</em> currently have visibility into is how often sensitive text or structured business context gets copied into external models, how many employees have autonomous agent workflows running locally or in personal cloud accounts, or how decisions influenced by those tools propagate into organizational artifacts such as product plans, pitch decks, specifications for NPC Manager, or incident postmortems.</p><p>The distinction between <strong>tools that are visible</strong> and <strong>tools that are invisible</strong> defines whether a cognitive path is accountable. If a roadmap iteration or architectural decision arises from a personal model’s suggestion, and no corporate record exists of that influence, then the <strong>organizational accountability is the employee</strong>. Decisions bind the company even if the reasoning is invisible. This is structurally different from past shadow IT problems because the employees’ <em>thinking process</em> itself becomes a component of how work is done.</p><p>This is not analogous to the introduction of past tools such as IDEs, search, or SaaS applications. Those technologies changed <em>speed</em> and <em>convenience</em> but did not systematically embed external reasoning artifacts into the cognitive workflows of employees. A spreadsheet, versioned code, or a search result can be inspected, audited, explained, and forensically reconstructed. By contrast, interactions with personal LLMs can change internal mental maps, introduce suggestions that are not logged, and incorporate external model assumptions that cannot be revisited or audited later. In this sense, LLMs alter not just <em>speed</em> but the <strong>structure of cognition</strong> as it intersects with institutional knowledge.</p><p>NPC Inc. has historically governed decisions. We require review processes for code merges, design sign-offs for major features, documentation artifacts for compliance, and approval gates for customer-impacting actions. But we have not explicitly governed <em>where and how</em> unstructured cognition occurs before the first artifact is created. This is because until recently, cognition did not leave traces outside controlled systems. Now it does, and companies everywhere are struggling with similar issues.</p><h2>Examples of Personal LLM Usage</h2><p>The following examples share a common pattern: useful cognitive acceleration and domain insight on the one hand, and <strong>a break in institutional alignment, traceability, and accountability</strong> on the other. Naive rules such as “no personal tools” are unenforceable and counterproductive; employees will circumvent them because they help get work done. Conversely, “anything goes” invites unmanaged risk where work with material business impact escapes governance.</p><ol><li><p><strong>Product Roadmap Narrative Drafted in a Personal LLM:</strong> An NPC Inc. senior product manager used their external LLM subscription to draft a roadmap. That narrative was refined iteratively over several weeks using personal prompts, then pasted into a corporate document as the official plan without an audit trail of how, precisely, the suggestions or priority assumptions were generated. Where this worked well was in accelerating ideation and surfacing alternative scenarios. The ambiguity emerged when engineering teams later tried to trace back a prioritization decision and the senior product manager could not articulate the detailed reason why certain trade-offs were made. This made it hard to justify assumptions in compliance and board reporting discussions.<br><br></p></li><li><p><strong>Local Model Debugging on Production Behavior:</strong> An NPC Inc. engineer downloaded a local model to assist in post-mortem analysis, and used agentic scripts to generate hypotheses about failure modes. It yielded useful insights and narrowed down possible root causes faster than NPC Inc’s company model. But the local model deviates from general guidance, voice, tone, and cultural context of the company model. This is creating tensions with other engineers. Other engineers are now copying this model for efficiency while losing the business context.<br><br></p></li><li><p><strong>Customer Facing Explanation Drafting:</strong> A sales engineer, trying to respond quickly to a technical RFP, used a popular public LLM to draft a compliance explanation. The text contained plausible legal language but included inaccuracies about jurisdiction-specific regulatory standards. The draft went through internal edits but the origin of the initial phrasing was not tracked. When Legal later reviewed the final document, they could not trace back which assertions were model-generated and which were human. This introduced legal risk because the liability for incorrect claims resides with NPC Inc., not the employee.<br><br></p></li><li><p><strong>Internal Agent Configuration with Undocumented Model Advice:</strong> A team is using a custom agent for internal scheduling optimization. They initially designed the agent’s logic with help from an external LLM, with which NPC Inc. does not contract. The agent worked quickly and with high accuracy, but its internal rationale included implicit assumptions derived from the LLMs system prompt, and is not localized to NPC Inc. Furthermore, NPC Inc does not have a commercial agreement; Using this LLM for enterprise usage may be off-license.</p></li></ol><h2>Risk Categories</h2><p>The risk landscape here must be described in terms that separate <em>type</em>, in addition to <em>severity</em>. There are at least 4 categories of risks, which are also visible in other enterprises, such as our customers.</p><ul><li><p><strong>Epistemic risk:</strong> Where the influence of external models on organizational knowledge cannot be reconstructed or justified. The risk here is that decisions are effectively unknowable in how they were shaped, compromising auditability.</p></li><li><p><strong>Operational risk:</strong> Where workflows depend on outputs that cannot be reproduced because they relied on undocumented personal tools, leading to breakdowns in reproducibility, continuity, and incident analysis.</p></li><li><p><strong>Legal risk:</strong> Where assertions, representations, or deliverables influenced by external tools create potential liability, especially in regulated environments where explainability is required for compliance or discovery.</p></li><li><p><strong>Cultural risk:</strong> Where shadow practices erode legitimacy and trust between employees and leadership, or across functions such as security and product. Without visible norms, people invent patterns that look like governance only in retrospect.</p></li></ul><h2>Recommended Operational Stance</h2><p>The central conceptual reframing we propose is to <strong>distinguish between “private exploratory cognition” and “organizationally binding cognition.”</strong> The former is the informal, internal thinking that precedes any deliverable artifact. It includes ideation, exploration, alternative scenario development, hypothesis generation, and rough drafts. The latter is any output that enters the institutional record: requirements, designs, decisions, commit logs, agent configurations, regulatory filings, client deliverables, or anything subject to compliance. Historically, organizations have implicitly drawn this boundary because the tools themselves left traces—emails, version control, documents. Now, the boundary is blurred because cognition leaves traces <em>outside</em> corporate systems.</p><p>This reframing foregrounds the insight that the <em>location and timing of traceability</em> matters more than the <em>identity of the tool</em>. If NPC Inc. can define where accountability begins, we can govern that transition without policing the entire landscape of cognitive work. <strong>Exploration and private ideation, including via personal models, remain within the individual domain, as long as outcomes that </strong><em><strong>bind</strong></em><strong> the organization cross explicit handoff protocols.</strong></p><p>Our recommended operating stance is grounded in this reframing:</p><ul><li><p>NPC Inc. <strong>explicitly allows</strong> private exploratory cognition using personal LLMs and agent tools, provided that nothing that constitutes an organizational commitment or deliverable is derived directly from them without going through established handoff and tracing protocols.</p></li><li><p>NPC Inc. <strong>explicitly governs</strong> the transition point where exploratory cognition contributes to organizational artifacts. At this boundary, employees must map influences, document sources (including models or agent prompts), and incorporate them into corporate trace and audit systems in a discoverable way.</p></li><li><p>NPC Inc. <strong>does not attempt to control</strong> how employees explore ideas on their own devices or external services. Enforcement of personal tool bans is unenforceable and counterproductive.</p></li><li><p><strong>Employees remain individually accountable</strong> when personal LLMs or agents are used to generate or materially shape work that enters organizationally binding cognition without following the required handoff, documentation, and review protocols, regardless of intent or productivity gains.</p></li></ul><p>In support of the operational stance above, a few items to clarify:</p><ul><li><p>Escalation from private exploration to institutional governance is triggered when work becomes <em>referenced in cross-functional artifacts, impacts product decisions, touches customer commitments, or intersects with compliance.</em></p></li><li><p>NPC Inc.’s stance aligns with NPC Manager’s external positioning as a governance and agent management platform because it emphasizes <em>traceable decision boundaries</em>, a core value both internally and for clients.</p></li><li><p>Our internal governance protocols can serve as a demonstration of the capabilities we build for customers: clarity about where and how agents and models contribute to accountable outcomes.</p></li></ul><p>These recommendations are structured so that they are enforceable through observable gates and audits rather than through prohibitions.</p><h2>Next Steps</h2><p>Overall, there will be ongoing risks. The precise mechanics of capturing influences without overwhelming employees with record-keeping burdens is open. The rate at which personal models evolve will create new kinds of artifacts that defy current logging. The best way to classify decision influence itself—rather than proxy artifacts—is yet unresolved.</p><p>Signals to watch in the next 12–24 months include:</p><ul><li><p>Increased employee transparency on personal LLM and agent usage</p></li><li><p>Patterns of data leakage through personal AI tools versus corporate tools</p></li><li><p>Disagreements about ownership of outputs influenced by external models</p></li><li><p>External regulatory pressures demanding explainability of AI-influenced decisions</p></li><li><p>Internal tensions where private exploration and binding decisions are misaligned</p></li></ul>